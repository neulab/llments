{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ca8370c",
   "metadata": {},
   "source": [
    "# CommunityLM\n",
    "\n",
    "This is a replication of the experiments from [CommunityLM](https://arxiv.org/abs/2209.07065) (Jiang et al. 2022), which probes partisan worldviews from language models, based on the [original repo](https://github.com/hjian42/communitylm).\n",
    "\n",
    "Running all the experiments on a single GPU takes about 3-4 hours.\n",
    "\n",
    "Before running the notebook, please install requirements and download the data.\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "bash download_data.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29563e5d-41b0-4f89-8d8b-a54b40f8dfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llments.lm.base.hugging_face import HuggingFaceLM, HuggingFaceLMFitter\n",
    "# from llments.lm.base.empirical import load_from_text_file\n",
    "from llments.eval.sentiment import HuggingFaceSentimentEvaluator\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from examples.community_lm.community_lm_constants import politician_feelings, groups_feelings, anes_df\n",
    "from examples.community_lm.community_lm_utils import generate_community_opinion, compute_group_stance\n",
    "\n",
    "device = 'cuda'  # change to 'mps' if you have a mac, or 'cuda:0' if you have an NVIDIA GPU "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4683f755",
   "metadata": {},
   "source": [
    "## Train a CommunityLM model (optional)\n",
    "\n",
    "The CommunityLM paper has released their pre-trained models on Hugging Face, so for the purpose of this notebook, we will use the pre-trained models. However, if you want to train a CommunityLM model from scratch, you can download training data to `data/{democrat,republican}-tweets.txt`, uncomment the following lines, and replace the `lm_name` variable in the following cell with `./data/{party}-twitter-gpt2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd430fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_model = HuggingFaceLM(\"gpt2\", device=device)\n",
    "# for party in ['democrat', 'republican']:\n",
    "#     dataset = load_from_text_file(f\"data/{party}-tweets.txt\")\n",
    "#     fit_model = HuggingFaceLMFitter.fit(base_model, dataset, output_dir=f\"data/{party}-twitter-gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0022efe",
   "metadata": {},
   "source": [
    "## Generate Opinions using CommunityLM\n",
    "\n",
    "The following code generates opinions using CommunityLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacd15ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "for run in range(1, 6):\n",
    "    for party in ['democrat', 'republican']:\n",
    "        # This uses the pre-trained communitylm, but you can uncomment if you trained your own model\n",
    "        lm_name = f'CommunityLM/{party}-twitter-gpt2'\n",
    "        # lm_name = f'./data/{party}-twitter-gpt2'\n",
    "        lm = HuggingFaceLM(lm_name, device=device)\n",
    "        for prompt_option in ['Prompt1', 'Prompt2', 'Prompt3', 'Prompt4']:\n",
    "            print(f'generating {party} opinion for {prompt_option} run {run}...')\n",
    "            output_path = f'output/CommunityLM_{party}-twitter-gpt2/run_{run}'\n",
    "            generate_community_opinion(lm, prompt_option, output_path, run)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348fc5e7-aad4-4d1a-9436-0ae83585e8bb",
   "metadata": {},
   "source": [
    "## Perform Group-level Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2049390",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing run_1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  33%|███▎      | 10/30 [00:37<01:22,  4.14s/it]--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mihirban/miniconda3/lib/python3.11/logging/__init__.py\", line 1110, in emit\n",
      "    msg = self.format(record)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/mihirban/miniconda3/lib/python3.11/logging/__init__.py\", line 953, in format\n",
      "    return fmt.format(record)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/mihirban/miniconda3/lib/python3.11/logging/__init__.py\", line 687, in format\n",
      "    record.message = record.getMessage()\n",
      "                     ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/mihirban/miniconda3/lib/python3.11/logging/__init__.py\", line 377, in getMessage\n",
      "    msg = msg % self.args\n",
      "          ~~~~^~~~~~~~~~~\n",
      "TypeError: not all arguments converted during string formatting\n",
      "Call stack:\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/home/mihirban/miniconda3/lib/python3.11/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/mihirban/miniconda3/lib/python3.11/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/mihirban/miniconda3/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/mihirban/miniconda3/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/home/mihirban/miniconda3/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/home/mihirban/miniconda3/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n",
      "    handle._run()\n",
      "  File \"/home/mihirban/miniconda3/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/mihirban/miniconda3/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/home/mihirban/miniconda3/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/home/mihirban/miniconda3/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/home/mihirban/miniconda3/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 359, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/home/mihirban/miniconda3/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/home/mihirban/miniconda3/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 446, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/home/mihirban/miniconda3/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/home/mihirban/miniconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/home/mihirban/miniconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/home/mihirban/miniconda3/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/mihirban/miniconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/home/mihirban/miniconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/home/mihirban/miniconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_1778592/4055455867.py\", line 6, in <module>\n",
      "    compute_group_stance(\n",
      "  File \"/home/mihirban/llments/examples/community_lm/community_lm_utils.py\", line 155, in compute_group_stance\n",
      "    sentiment_vals = evaluator.evaluate_batch(\n",
      "  File \"/home/mihirban/llments/llments/eval/sentiment.py\", line 105, in evaluate_batch\n",
      "    for x in self.sentiment_pipeline(minibatch)\n",
      "  File \"/home/mihirban/miniconda3/lib/python3.11/site-packages/transformers/pipelines/text_classification.py\", line 156, in __call__\n",
      "    result = super().__call__(*inputs, **kwargs)\n",
      "  File \"/home/mihirban/miniconda3/lib/python3.11/site-packages/transformers/pipelines/base.py\", line 1167, in __call__\n",
      "    logger.warning_once(\n",
      "  File \"/home/mihirban/miniconda3/lib/python3.11/site-packages/transformers/utils/logging.py\", line 329, in warning_once\n",
      "    self.warning(*args, **kwargs)\n",
      "Message: 'You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset'\n",
      "Arguments: (<class 'UserWarning'>,)\n",
      "Processing questions: 100%|██████████| 30/30 [02:01<00:00,  4.05s/it]\n",
      "Processing questions: 100%|██████████| 30/30 [02:05<00:00,  4.18s/it]\n",
      "Processing questions: 100%|██████████| 30/30 [02:05<00:00,  4.18s/it]\n",
      "Processing questions: 100%|██████████| 30/30 [02:05<00:00,  4.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing run_2 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions: 100%|██████████| 30/30 [02:04<00:00,  4.16s/it]\n",
      "Processing questions: 100%|██████████| 30/30 [02:05<00:00,  4.17s/it]\n",
      "Processing questions: 100%|██████████| 30/30 [02:05<00:00,  4.18s/it]\n",
      "Processing questions: 100%|██████████| 30/30 [02:05<00:00,  4.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing run_3 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions: 100%|██████████| 30/30 [02:05<00:00,  4.17s/it]\n",
      "Processing questions: 100%|██████████| 30/30 [02:05<00:00,  4.17s/it]\n",
      "Processing questions: 100%|██████████| 30/30 [02:05<00:00,  4.17s/it]\n",
      "Processing questions: 100%|██████████| 30/30 [02:04<00:00,  4.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing run_4 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions: 100%|██████████| 30/30 [02:05<00:00,  4.17s/it]\n",
      "Processing questions: 100%|██████████| 30/30 [02:05<00:00,  4.17s/it]\n",
      "Processing questions: 100%|██████████| 30/30 [02:05<00:00,  4.17s/it]\n",
      "Processing questions: 100%|██████████| 30/30 [02:05<00:00,  4.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing run_5 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions: 100%|██████████| 30/30 [02:05<00:00,  4.17s/it]\n",
      "Processing questions:  13%|█▎        | 4/30 [00:16<01:48,  4.18s/it]"
     ]
    }
   ],
   "source": [
    "evaluator = HuggingFaceSentimentEvaluator(\n",
    "    \"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
    "    device=device\n",
    ")\n",
    "for party in ['democrat', 'republican']:\n",
    "    compute_group_stance(\n",
    "        evaluator=evaluator,\n",
    "        data_folder=f'output/CommunityLM_{party}-twitter-gpt2',\n",
    "        output_filename=f'output/CommunityLM_{party}-twitter-gpt2/stance_prediction.csv',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec53be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dem = pd.read_csv(\"output/CommunityLM_democrat-twitter-gpt2/stance_prediction.csv\")\n",
    "df_repub = pd.read_csv(\"output/CommunityLM_republican-twitter-gpt2/stance_prediction.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6017a1d8-ae02-4adb-b3af-3d19911a61a2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Preparing ANES2020 Questions\n",
    "\n",
    "This is data from the American National Election Study (ANES)\n",
    "\n",
    "Website: https://electionstudies.org/\n",
    "Email:   anes@electionstudies.org\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e5cf0c-3f2c-4cae-806a-3798f8138664",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/anes_pilot_2020ets_csv.csv\")\n",
    "\n",
    "print(f\"Number of Rows Total {df.shape}\")\n",
    "\n",
    "# only look self identified partisans 2144/3080. 1: Republican; 2: Democrat\n",
    "df = df[df.pid1r < 3]\n",
    "df.pid1r = df.pid1r.map({1: \"Republican\", 2: \"Democrat\"})\n",
    "print(f\"Number of Rows for Partisans {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976e4ba7-6c58-4445-9522-fe844342df1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 999 stands for missing values and 'pid1r' is the partisanship\n",
    "df_politician_results = df[['pid1r']+politician_feelings+groups_feelings].replace(999, np.nan).groupby(\"pid1r\").mean().T\n",
    "df_politician_results['is_repub_leading'] = (df_politician_results.Republican > df_politician_results.Democrat)\n",
    "# df_politician_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34e06de-bfdd-4475-a4d6-47a17d627bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_politician_results['Prompt1'] = anes_df['Prompt1'].to_list()\n",
    "df_politician_results['Prompt2'] = anes_df['Prompt2'].to_list()\n",
    "df_politician_results['Prompt3'] = anes_df['Prompt3'].to_list()\n",
    "df_politician_results['Prompt4'] = anes_df['Prompt4'].to_list()\n",
    "\n",
    "df_politician_results['pid'] = df_politician_results.index\n",
    "df_politician_results.to_csv(\"output/anes2020_pilot_prompt_probing_ft.csv\", index=False)\n",
    "# df_politician_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabcbbde-38a0-4e7c-a0a3-93034ce589c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_politician_results['diff'] = (df_politician_results.Democrat-df_politician_results.Republican).apply(abs)\n",
    "df_politician_results.sort_values(by=['diff'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fe8318-5836-448c-bb50-301845732f53",
   "metadata": {},
   "source": [
    "## Evaluate fine-tuned GPT-2 CommunityLM models\n",
    "\n",
    "This evaluates the sentiment of the completions generated by each model according to a sentiment classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f6811b-099e-4ba5-993c-3b7ada968f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "def compute_scores(df_anes, df_dem, df_repub):\n",
    "    df_repub['prediction'] = (df_repub['group_sentiment'] > df_dem['group_sentiment'])\n",
    "\n",
    "    gold_labels = df_anes.is_repub_leading.astype(int).values\n",
    "    rows = []\n",
    "    for run in range(1, 6):\n",
    "        run = \"run_{}\".format(run)\n",
    "        for prompt_format in range(1, 5):\n",
    "            prompt_format = \"Prompt{}\".format(prompt_format)\n",
    "            df_ = df_repub[(df_repub.run == run) & (df_repub.prompt_format == prompt_format)]\n",
    "            pred_labels = df_.prediction.astype(int).values\n",
    "            acc = accuracy_score(gold_labels, pred_labels) \n",
    "            p, r, f1, _ = precision_recall_fscore_support(gold_labels, pred_labels, average='weighted')\n",
    "            rows.append([run, prompt_format, acc, p, r, f1])\n",
    "    df_scores = pd.DataFrame(rows, columns=[\"run\", \"prompt_format\", \"accuracy\", \"precision\", \"recall\", \"f1\"])\n",
    "    return df_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d429b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_repub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a6ef2b-ff35-49dc-92a7-0fb984fed6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"output/anes2020_pilot_prompt_probing_ft.csv\")\n",
    "df_scores = compute_scores(df, df_dem, df_repub)\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200fb627-57f4-4d73-a375-bb87e95923c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract gold ranks\n",
    "df_politician_results = df_politician_results.sort_values(by=[\"pid\"])\n",
    "gold_dem_rank = df_politician_results['Democrat'].rank().values\n",
    "gold_repub_rank = df_politician_results['Republican'].rank().values\n",
    "gold_repub_rank\n",
    "\n",
    "from scipy import stats\n",
    "def extract_ranking(df_):\n",
    "    df_ = df_.sort_values(by=['question'])\n",
    "    return df_[df_.prompt_format == \"Prompt4\"].groupby(['question']).group_sentiment.mean().rank().values\n",
    "\n",
    "dem_rank = extract_ranking(df_dem)\n",
    "repub_rank = extract_ranking(df_repub)\n",
    "\n",
    "gold_dem_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69f6bc3-b8d2-44ad-9aab-222653191c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot the rankings\n",
    "\n",
    "def extract_ranking_for_politicians(df_):\n",
    "    df_ = df_[df_.question.isin(politician_feelings)]\n",
    "    df_ = df_.sort_values(by=['question', 'run'])\n",
    "    return df_[df_.prompt_format == \"Prompt4\"]\n",
    "\n",
    "df_politician_results = df_politician_results[df_politician_results.pid.isin(politician_feelings)].sort_values(by=['pid'])\n",
    "df_politician_results['short_name'] = df_politician_results.Prompt1.apply(lambda x: x.split(\" \")[-1])\n",
    "\n",
    "dem_politician_rank = extract_ranking_for_politicians(df_dem)\n",
    "df_avg = dem_politician_rank.groupby(\"question\").group_sentiment.mean().reset_index()\n",
    "df_avg['group_avg_sentiment'] = df_avg['group_sentiment']\n",
    "del df_avg[\"group_sentiment\"]\n",
    "dem_politician_rank = dem_politician_rank.merge(df_politician_results, left_on=\"question\", right_on=\"pid\")\n",
    "dem_politician_rank = dem_politician_rank.merge(df_avg, on=\"question\")\n",
    "\n",
    "\n",
    "repub_politician_rank = extract_ranking_for_politicians(df_repub)\n",
    "df_avg = repub_politician_rank.groupby(\"question\").group_sentiment.mean().reset_index()\n",
    "df_avg['group_avg_sentiment'] = df_avg['group_sentiment']\n",
    "del df_avg[\"group_sentiment\"]\n",
    "repub_politician_rank = repub_politician_rank.merge(df_politician_results, left_on=\"question\", right_on=\"pid\")\n",
    "repub_politician_rank = repub_politician_rank.merge(df_avg, on=\"question\")\n",
    "\n",
    "\n",
    "dem_politician_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015bb056-a742-49a1-97a7-dda18d203ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_politician_results.to_csv(\"rank_plots.csv\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(rc={'figure.figsize':(5,5)})\n",
    "\n",
    "palette = sns.color_palette(\"Blues\",n_colors=20)\n",
    "palette.reverse()\n",
    "\n",
    "ax = sns.barplot(data=dem_politician_rank.sort_values(by=\"group_avg_sentiment\", ascending=False), x=\"group_sentiment\", y=\"short_name\", palette=palette, estimator=np.mean, ci=90)\n",
    "\n",
    "ax.set_xlabel(None, fontsize=15)\n",
    "ax.set_ylabel(None)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('rankings/finetuned_gpt2_pred_dem_rank.png', bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3b9c6c-a2e1-43b4-8463-caaff9653fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(5,5)})\n",
    "\n",
    "palette = sns.color_palette(\"Blues\",n_colors=20)\n",
    "palette.reverse()\n",
    "\n",
    "ax = sns.barplot(data=dem_politician_rank.sort_values(by=\"Democrat\", ascending=False), x=\"Democrat\", y=\"short_name\", palette=palette)\n",
    "\n",
    "ax.set_xlabel(None, fontsize=15)\n",
    "ax.set_ylabel(None)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('rankings/gold_dem_rank.png', bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc980b7-aa69-4cbe-8778-f57b463fc909",
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = sns.color_palette(\"Reds\", n_colors=20)\n",
    "palette.reverse()\n",
    "\n",
    "ax = sns.barplot(data=repub_politician_rank.sort_values(by=\"group_avg_sentiment\", ascending=False), x=\"group_sentiment\", y=\"short_name\", palette=palette)\n",
    "\n",
    "ax.set_xlabel(None, fontsize=15)\n",
    "ax.set_ylabel(None)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('rankings/finetuned_gpt2_pred_repub_rank.png', bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1dd3c2-dda4-482e-99b6-6ef4316155ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = sns.color_palette(\"Reds\", n_colors=20)\n",
    "palette.reverse()\n",
    "\n",
    "ax = sns.barplot(data=repub_politician_rank.sort_values(by=\"Republican\", ascending=False), x=\"Republican\", y=\"short_name\", palette=palette)\n",
    "\n",
    "ax.set_xlabel(None, fontsize=15)\n",
    "ax.set_ylabel(None)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('rankings/gold_repub_rank.png', bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5082d3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
