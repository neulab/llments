{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ca8370c",
   "metadata": {},
   "source": [
    "# CommunityLM\n",
    "\n",
    "This is a replication of the experiments from [CommunityLM](https://arxiv.org/abs/2209.07065) (Jiang et al. 2022), which probes partisan worldviews from language models, based on the [original repo](https://github.com/hjian42/communitylm).\n",
    "\n",
    "Running all the experiments on a single GPU takes about 3-4 hours.\n",
    "\n",
    "Before running the notebook, please install requirements and download the data.\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "bash download_data.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29563e5d-41b0-4f89-8d8b-a54b40f8dfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llments.lm.base.hugging_face import HuggingFaceLM, HuggingFaceLMFitter\n",
    "from llments.lm.base.dataset_lm import load_from_text_file\n",
    "from llments.eval.sentiment import HuggingFaceSentimentEvaluator\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from community_lm_constants import politician_feelings, groups_feelings, anes_df\n",
    "from community_lm_utils import generate_community_opinion, compute_group_stance\n",
    "\n",
    "device = 'cuda:0'  # change to 'mps' if you have a mac, or 'cuda:0' if you have an NVIDIA GPU "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4683f755",
   "metadata": {},
   "source": [
    "## Train a CommunityLM model (optional)\n",
    "\n",
    "The CommunityLM paper has released their pre-trained models on Hugging Face, so for the purpose of this notebook, we will use the pre-trained models. However, if you want to train a CommunityLM model from scratch, you can download training data to `data/{democrat,republican}-tweets.txt`, uncomment the following lines, and replace the `lm_name` variable in the following cell with `./data/{party}-twitter-gpt2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbd430fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "You need to install 'transformers' and 'torch' packages to use this function.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/llments/llments/lm/base/hugging_face.py:260\u001b[0m, in \u001b[0;36mHuggingFaceLMFitter.fit\u001b[0;34m(cls, base, target, eval_target, batch_size, training_steps, output_dir, logging_dir, do_train, do_eval, learning_rate, warmup_steps, max_grad_norm, evalution_strategy, eval_steps, prediction_loss_only, optim, logging_steps, save_steps, lora_r, lora_alpha)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 260\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m    262\u001b[0m         DataCollatorForLanguageModeling,\n\u001b[1;32m    263\u001b[0m         Trainer,\n\u001b[1;32m    264\u001b[0m         TrainingArguments,\n\u001b[1;32m    265\u001b[0m     )\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'datasets'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m party \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdem\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrepub\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m      3\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m load_from_text_file(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparty\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_4.7M_tweets.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m     fit_model \u001b[38;5;241m=\u001b[39m \u001b[43mHuggingFaceLMFitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mparty\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m-twitter-gpt2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/llments/llments/lm/base/hugging_face.py:267\u001b[0m, in \u001b[0;36mHuggingFaceLMFitter.fit\u001b[0;34m(cls, base, target, eval_target, batch_size, training_steps, output_dir, logging_dir, do_train, do_eval, learning_rate, warmup_steps, max_grad_norm, evalution_strategy, eval_steps, prediction_loss_only, optim, logging_steps, save_steps, lora_r, lora_alpha)\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m    262\u001b[0m         DataCollatorForLanguageModeling,\n\u001b[1;32m    263\u001b[0m         Trainer,\n\u001b[1;32m    264\u001b[0m         TrainingArguments,\n\u001b[1;32m    265\u001b[0m     )\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m--> 267\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m    268\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou need to install \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformers\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m packages to use this \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    269\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    270\u001b[0m     )\n\u001b[1;32m    272\u001b[0m \u001b[38;5;66;03m# Generate data and prepare training dataset\u001b[39;00m\n\u001b[1;32m    273\u001b[0m samples \u001b[38;5;241m=\u001b[39m target\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m    274\u001b[0m     condition\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    275\u001b[0m     do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    276\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m,\n\u001b[1;32m    277\u001b[0m     num_return_sequences\u001b[38;5;241m=\u001b[39mbatch_size \u001b[38;5;241m*\u001b[39m training_steps,\n\u001b[1;32m    278\u001b[0m )\n",
      "\u001b[0;31mImportError\u001b[0m: You need to install 'transformers' and 'torch' packages to use this function."
     ]
    }
   ],
   "source": [
    "base_model = HuggingFaceLM(\"gpt2\", device=device)\n",
    "for party in ['dem', 'repub']:\n",
    "    dataset = load_from_text_file(f\"data/{party}_4.7M_tweets.txt\")\n",
    "    fit_model = HuggingFaceLMFitter.fit(base_model, dataset, output_dir=f\"data/{party}-twitter-gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0022efe",
   "metadata": {},
   "source": [
    "## Generate Opinions using CommunityLM\n",
    "\n",
    "The following code generates opinions using CommunityLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bacd15ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/capstone/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating democrat opinion for Prompt1 run 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating opinions:   0%|          | 0/30 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:   3%|▎         | 1/30 [00:04<02:13,  4.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:   7%|▋         | 2/30 [00:07<01:37,  3.46s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  10%|█         | 3/30 [00:09<01:23,  3.11s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  13%|█▎        | 4/30 [00:12<01:16,  2.94s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  17%|█▋        | 5/30 [00:15<01:11,  2.85s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  20%|██        | 6/30 [00:17<01:06,  2.77s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  23%|██▎       | 7/30 [00:20<01:02,  2.73s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  27%|██▋       | 8/30 [00:23<00:59,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  30%|███       | 9/30 [00:25<00:56,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  33%|███▎      | 10/30 [00:28<00:53,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  37%|███▋      | 11/30 [00:31<00:51,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  40%|████      | 12/30 [00:33<00:48,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  43%|████▎     | 13/30 [00:36<00:45,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  47%|████▋     | 14/30 [00:39<00:42,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  50%|█████     | 15/30 [00:41<00:39,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  53%|█████▎    | 16/30 [00:44<00:36,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  57%|█████▋    | 17/30 [00:47<00:34,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  60%|██████    | 18/30 [00:49<00:31,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  63%|██████▎   | 19/30 [00:52<00:29,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  67%|██████▋   | 20/30 [00:55<00:26,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  70%|███████   | 21/30 [00:57<00:24,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  73%|███████▎  | 22/30 [01:00<00:21,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  77%|███████▋  | 23/30 [01:03<00:18,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  80%|████████  | 24/30 [01:05<00:15,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  83%|████████▎ | 25/30 [01:08<00:13,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  87%|████████▋ | 26/30 [01:11<00:10,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  90%|█████████ | 27/30 [01:14<00:08,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  93%|█████████▎| 28/30 [01:16<00:05,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  97%|█████████▋| 29/30 [01:19<00:02,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions: 100%|██████████| 30/30 [01:22<00:00,  2.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating democrat opinion for Prompt2 run 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating opinions:   0%|          | 0/30 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:   3%|▎         | 1/30 [00:02<01:17,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:   7%|▋         | 2/30 [00:05<01:14,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  10%|█         | 3/30 [00:07<01:11,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  13%|█▎        | 4/30 [00:10<01:09,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  17%|█▋        | 5/30 [00:13<01:06,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  20%|██        | 6/30 [00:15<01:03,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  23%|██▎       | 7/30 [00:18<01:00,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  27%|██▋       | 8/30 [00:21<00:57,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  30%|███       | 9/30 [00:23<00:55,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  33%|███▎      | 10/30 [00:26<00:52,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  37%|███▋      | 11/30 [00:29<00:50,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  40%|████      | 12/30 [00:31<00:47,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  43%|████▎     | 13/30 [00:34<00:44,  2.59s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  47%|████▋     | 14/30 [00:36<00:41,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  50%|█████     | 15/30 [00:39<00:39,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  53%|█████▎    | 16/30 [00:42<00:36,  2.59s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  57%|█████▋    | 17/30 [00:44<00:34,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  60%|██████    | 18/30 [00:47<00:31,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  63%|██████▎   | 19/30 [00:50<00:29,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  67%|██████▋   | 20/30 [00:52<00:26,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  70%|███████   | 21/30 [00:55<00:23,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  73%|███████▎  | 22/30 [00:58<00:21,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  77%|███████▋  | 23/30 [01:00<00:18,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  80%|████████  | 24/30 [01:03<00:15,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  83%|████████▎ | 25/30 [01:05<00:13,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  87%|████████▋ | 26/30 [01:08<00:10,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  90%|█████████ | 27/30 [01:11<00:07,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  93%|█████████▎| 28/30 [01:13<00:05,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  97%|█████████▋| 29/30 [01:16<00:02,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions: 100%|██████████| 30/30 [01:19<00:00,  2.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating democrat opinion for Prompt3 run 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating opinions:   0%|          | 0/30 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:   3%|▎         | 1/30 [00:02<01:16,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:   7%|▋         | 2/30 [00:05<01:13,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  10%|█         | 3/30 [00:07<01:11,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  13%|█▎        | 4/30 [00:10<01:08,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  17%|█▋        | 5/30 [00:13<01:05,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  20%|██        | 6/30 [00:15<01:02,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  23%|██▎       | 7/30 [00:18<00:59,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  27%|██▋       | 8/30 [00:20<00:57,  2.59s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  30%|███       | 9/30 [00:23<00:54,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  33%|███▎      | 10/30 [00:26<00:52,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  37%|███▋      | 11/30 [00:28<00:49,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  40%|████      | 12/30 [00:31<00:47,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  43%|████▎     | 13/30 [00:34<00:44,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  47%|████▋     | 14/30 [00:36<00:41,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  50%|█████     | 15/30 [00:39<00:39,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  53%|█████▎    | 16/30 [00:41<00:36,  2.58s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  57%|█████▋    | 17/30 [00:44<00:33,  2.60s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  60%|██████    | 18/30 [00:47<00:31,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  63%|██████▎   | 19/30 [00:49<00:28,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  67%|██████▋   | 20/30 [00:52<00:26,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  70%|███████   | 21/30 [00:54<00:23,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  73%|███████▎  | 22/30 [00:57<00:21,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  77%|███████▋  | 23/30 [01:00<00:18,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  80%|████████  | 24/30 [01:02<00:15,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  83%|████████▎ | 25/30 [01:05<00:13,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  87%|████████▋ | 26/30 [01:08<00:10,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  90%|█████████ | 27/30 [01:10<00:07,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  93%|█████████▎| 28/30 [01:13<00:05,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  97%|█████████▋| 29/30 [01:15<00:02,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions: 100%|██████████| 30/30 [01:18<00:00,  2.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating democrat opinion for Prompt4 run 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating opinions:   0%|          | 0/30 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:   3%|▎         | 1/30 [00:02<01:16,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:   7%|▋         | 2/30 [00:05<01:13,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  10%|█         | 3/30 [00:07<01:11,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  13%|█▎        | 4/30 [00:10<01:08,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  17%|█▋        | 5/30 [00:13<01:05,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  20%|██        | 6/30 [00:15<01:02,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  23%|██▎       | 7/30 [00:18<00:59,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  27%|██▋       | 8/30 [00:20<00:57,  2.59s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  30%|███       | 9/30 [00:23<00:54,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  33%|███▎      | 10/30 [00:26<00:52,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  37%|███▋      | 11/30 [00:28<00:49,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  40%|████      | 12/30 [00:31<00:47,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  43%|████▎     | 13/30 [00:33<00:43,  2.56s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  47%|████▋     | 14/30 [00:36<00:41,  2.58s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  50%|█████     | 15/30 [00:39<00:38,  2.58s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  53%|█████▎    | 16/30 [00:41<00:35,  2.57s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  57%|█████▋    | 17/30 [00:44<00:33,  2.59s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  60%|██████    | 18/30 [00:46<00:31,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  63%|██████▎   | 19/30 [00:49<00:28,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  67%|██████▋   | 20/30 [00:52<00:26,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  70%|███████   | 21/30 [00:54<00:23,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  73%|███████▎  | 22/30 [00:57<00:21,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  77%|███████▋  | 23/30 [01:00<00:18,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  80%|████████  | 24/30 [01:02<00:15,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  83%|████████▎ | 25/30 [01:05<00:13,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  87%|████████▋ | 26/30 [01:07<00:10,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  90%|█████████ | 27/30 [01:10<00:07,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  93%|█████████▎| 28/30 [01:13<00:05,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  97%|█████████▋| 29/30 [01:15<00:02,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions: 100%|██████████| 30/30 [01:18<00:00,  2.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating republican opinion for Prompt1 run 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating opinions:   0%|          | 0/30 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:   3%|▎         | 1/30 [00:02<01:18,  2.71s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:   7%|▋         | 2/30 [00:05<01:15,  2.70s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  10%|█         | 3/30 [00:08<01:13,  2.71s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  13%|█▎        | 4/30 [00:10<01:10,  2.71s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  17%|█▋        | 5/30 [00:13<01:07,  2.71s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  20%|██        | 6/30 [00:16<01:04,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  23%|██▎       | 7/30 [00:18<01:01,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  27%|██▋       | 8/30 [00:21<00:58,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  30%|███       | 9/30 [00:24<00:56,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  33%|███▎      | 10/30 [00:26<00:53,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  37%|███▋      | 11/30 [00:29<00:50,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  40%|████      | 12/30 [00:32<00:48,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  43%|████▎     | 13/30 [00:34<00:44,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  47%|████▋     | 14/30 [00:37<00:42,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  50%|█████     | 15/30 [00:40<00:39,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  53%|█████▎    | 16/30 [00:42<00:36,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  57%|█████▋    | 17/30 [00:45<00:34,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  60%|██████    | 18/30 [00:48<00:31,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  63%|██████▎   | 19/30 [00:50<00:29,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  67%|██████▋   | 20/30 [00:53<00:26,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  70%|███████   | 21/30 [00:56<00:24,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  73%|███████▎  | 22/30 [00:58<00:21,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  77%|███████▋  | 23/30 [01:01<00:18,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  80%|████████  | 24/30 [01:04<00:16,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  83%|████████▎ | 25/30 [01:06<00:13,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  87%|████████▋ | 26/30 [01:09<00:10,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  90%|█████████ | 27/30 [01:12<00:08,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  93%|█████████▎| 28/30 [01:14<00:05,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  97%|█████████▋| 29/30 [01:17<00:02,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions: 100%|██████████| 30/30 [01:20<00:00,  2.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating republican opinion for Prompt2 run 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating opinions:   0%|          | 0/30 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:   3%|▎         | 1/30 [00:02<01:17,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:   7%|▋         | 2/30 [00:05<01:14,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  10%|█         | 3/30 [00:08<01:12,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  13%|█▎        | 4/30 [00:10<01:09,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  17%|█▋        | 5/30 [00:13<01:06,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  20%|██        | 6/30 [00:15<01:03,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  23%|██▎       | 7/30 [00:18<01:00,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  27%|██▋       | 8/30 [00:21<00:57,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  30%|███       | 9/30 [00:23<00:55,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  33%|███▎      | 10/30 [00:26<00:53,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  37%|███▋      | 11/30 [00:29<00:50,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  40%|████      | 12/30 [00:31<00:47,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  43%|████▎     | 13/30 [00:34<00:44,  2.59s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  47%|████▋     | 14/30 [00:36<00:41,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  50%|█████     | 15/30 [00:39<00:39,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  53%|█████▎    | 16/30 [00:42<00:36,  2.60s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  57%|█████▋    | 17/30 [00:44<00:34,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  60%|██████    | 18/30 [00:47<00:31,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  63%|██████▎   | 19/30 [00:50<00:29,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  67%|██████▋   | 20/30 [00:52<00:26,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  70%|███████   | 21/30 [00:55<00:23,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  73%|███████▎  | 22/30 [00:58<00:21,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  77%|███████▋  | 23/30 [01:00<00:18,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  80%|████████  | 24/30 [01:03<00:15,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  83%|████████▎ | 25/30 [01:06<00:13,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  87%|████████▋ | 26/30 [01:08<00:10,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  90%|█████████ | 27/30 [01:11<00:07,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  93%|█████████▎| 28/30 [01:14<00:05,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  97%|█████████▋| 29/30 [01:16<00:02,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions: 100%|██████████| 30/30 [01:19<00:00,  2.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating republican opinion for Prompt3 run 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating opinions:   0%|          | 0/30 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:   3%|▎         | 1/30 [00:02<01:16,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:   7%|▋         | 2/30 [00:05<01:13,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  10%|█         | 3/30 [00:07<01:11,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  13%|█▎        | 4/30 [00:10<01:08,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  17%|█▋        | 5/30 [00:13<01:06,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  20%|██        | 6/30 [00:15<01:02,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  23%|██▎       | 7/30 [00:18<01:00,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  27%|██▋       | 8/30 [00:20<00:57,  2.60s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  30%|███       | 9/30 [00:23<00:54,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  33%|███▎      | 10/30 [00:26<00:52,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  37%|███▋      | 11/30 [00:28<00:49,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  40%|████      | 12/30 [00:31<00:47,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  43%|████▎     | 13/30 [00:33<00:43,  2.57s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  47%|████▋     | 14/30 [00:36<00:41,  2.58s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  50%|█████     | 15/30 [00:39<00:38,  2.59s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  53%|█████▎    | 16/30 [00:41<00:35,  2.57s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  57%|█████▋    | 17/30 [00:44<00:33,  2.59s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  60%|██████    | 18/30 [00:46<00:31,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  63%|██████▎   | 19/30 [00:49<00:28,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  67%|██████▋   | 20/30 [00:52<00:26,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  70%|███████   | 21/30 [00:54<00:23,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  73%|███████▎  | 22/30 [00:57<00:21,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  77%|███████▋  | 23/30 [01:00<00:18,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  80%|████████  | 24/30 [01:02<00:15,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  83%|████████▎ | 25/30 [01:05<00:13,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  87%|████████▋ | 26/30 [01:08<00:10,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  90%|█████████ | 27/30 [01:10<00:07,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  93%|█████████▎| 28/30 [01:13<00:05,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  97%|█████████▋| 29/30 [01:15<00:02,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions: 100%|██████████| 30/30 [01:18<00:00,  2.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating republican opinion for Prompt4 run 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating opinions:   0%|          | 0/30 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:   3%|▎         | 1/30 [00:02<01:16,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:   7%|▋         | 2/30 [00:05<01:13,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  10%|█         | 3/30 [00:07<01:11,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  13%|█▎        | 4/30 [00:10<01:08,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  17%|█▋        | 5/30 [00:13<01:06,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  20%|██        | 6/30 [00:15<01:02,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  23%|██▎       | 7/30 [00:18<01:00,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  27%|██▋       | 8/30 [00:20<00:57,  2.60s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  30%|███       | 9/30 [00:23<00:54,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  33%|███▎      | 10/30 [00:26<00:52,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  37%|███▋      | 11/30 [00:28<00:49,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  40%|████      | 12/30 [00:31<00:47,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  43%|████▎     | 13/30 [00:33<00:43,  2.57s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  47%|████▋     | 14/30 [00:36<00:41,  2.58s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  50%|█████     | 15/30 [00:39<00:38,  2.59s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  53%|█████▎    | 16/30 [00:41<00:35,  2.57s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  57%|█████▋    | 17/30 [00:44<00:33,  2.59s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  60%|██████    | 18/30 [00:46<00:31,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  63%|██████▎   | 19/30 [00:49<00:28,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  67%|██████▋   | 20/30 [00:52<00:26,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  70%|███████   | 21/30 [00:54<00:23,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  73%|███████▎  | 22/30 [00:57<00:21,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  77%|███████▋  | 23/30 [01:00<00:18,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  80%|████████  | 24/30 [01:02<00:15,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  83%|████████▎ | 25/30 [01:05<00:13,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  87%|████████▋ | 26/30 [01:08<00:10,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  90%|█████████ | 27/30 [01:10<00:07,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  93%|█████████▎| 28/30 [01:13<00:05,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  97%|█████████▋| 29/30 [01:15<00:02,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions: 100%|██████████| 30/30 [01:18<00:00,  2.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating democrat opinion for Prompt1 run 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating opinions:   0%|          | 0/30 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:   3%|▎         | 1/30 [00:02<01:18,  2.70s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:   7%|▋         | 2/30 [00:05<01:15,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  10%|█         | 3/30 [00:08<01:12,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  13%|█▎        | 4/30 [00:10<01:10,  2.70s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  17%|█▋        | 5/30 [00:13<01:07,  2.70s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  20%|██        | 6/30 [00:16<01:04,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  23%|██▎       | 7/30 [00:18<01:01,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  27%|██▋       | 8/30 [00:21<00:58,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  30%|███       | 9/30 [00:24<00:55,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  33%|███▎      | 10/30 [00:26<00:53,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  37%|███▋      | 11/30 [00:29<00:50,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  40%|████      | 12/30 [00:32<00:48,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  43%|████▎     | 13/30 [00:34<00:44,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  47%|████▋     | 14/30 [00:37<00:42,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  50%|█████     | 15/30 [00:39<00:39,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  53%|█████▎    | 16/30 [00:42<00:36,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  57%|█████▋    | 17/30 [00:45<00:34,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  60%|██████    | 18/30 [00:47<00:31,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  63%|██████▎   | 19/30 [00:50<00:29,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  67%|██████▋   | 20/30 [00:53<00:26,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  70%|███████   | 21/30 [00:56<00:24,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  73%|███████▎  | 22/30 [00:58<00:21,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  77%|███████▋  | 23/30 [01:01<00:18,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  80%|████████  | 24/30 [01:04<00:16,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  83%|████████▎ | 25/30 [01:06<00:13,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  87%|████████▋ | 26/30 [01:09<00:10,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  90%|█████████ | 27/30 [01:12<00:08,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  93%|█████████▎| 28/30 [01:14<00:05,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  97%|█████████▋| 29/30 [01:17<00:02,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions: 100%|██████████| 30/30 [01:20<00:00,  2.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating democrat opinion for Prompt2 run 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating opinions:   0%|          | 0/30 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:   3%|▎         | 1/30 [00:02<01:17,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:   7%|▋         | 2/30 [00:05<01:14,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  10%|█         | 3/30 [00:08<01:12,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  13%|█▎        | 4/30 [00:10<01:09,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  17%|█▋        | 5/30 [00:13<01:06,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  20%|██        | 6/30 [00:15<01:03,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  23%|██▎       | 7/30 [00:18<01:00,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  27%|██▋       | 8/30 [00:21<00:57,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  30%|███       | 9/30 [00:23<00:55,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  33%|███▎      | 10/30 [00:26<00:53,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  37%|███▋      | 11/30 [00:29<00:50,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  40%|████      | 12/30 [00:31<00:47,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  43%|████▎     | 13/30 [00:34<00:44,  2.60s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  47%|████▋     | 14/30 [00:36<00:41,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  50%|█████     | 15/30 [00:39<00:39,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  53%|█████▎    | 16/30 [00:42<00:36,  2.60s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  57%|█████▋    | 17/30 [00:44<00:34,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  60%|██████    | 18/30 [00:47<00:31,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  63%|██████▎   | 19/30 [00:50<00:29,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  67%|██████▋   | 20/30 [00:52<00:26,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  70%|███████   | 21/30 [00:55<00:23,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  73%|███████▎  | 22/30 [00:58<00:21,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  77%|███████▋  | 23/30 [01:00<00:18,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  80%|████████  | 24/30 [01:03<00:15,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  83%|████████▎ | 25/30 [01:06<00:13,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  87%|████████▋ | 26/30 [01:08<00:10,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  90%|█████████ | 27/30 [01:11<00:07,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  93%|█████████▎| 28/30 [01:14<00:05,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  97%|█████████▋| 29/30 [01:16<00:02,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions: 100%|██████████| 30/30 [01:19<00:00,  2.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating democrat opinion for Prompt3 run 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating opinions:   0%|          | 0/30 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:   3%|▎         | 1/30 [00:02<01:16,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:   7%|▋         | 2/30 [00:05<01:13,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  10%|█         | 3/30 [00:07<01:11,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  13%|█▎        | 4/30 [00:10<01:08,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  17%|█▋        | 5/30 [00:13<01:06,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  20%|██        | 6/30 [00:15<01:02,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  23%|██▎       | 7/30 [00:18<01:00,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  27%|██▋       | 8/30 [00:20<00:57,  2.60s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  30%|███       | 9/30 [00:23<00:54,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  33%|███▎      | 10/30 [00:26<00:52,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  37%|███▋      | 11/30 [00:28<00:49,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  40%|████      | 12/30 [00:31<00:47,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  43%|████▎     | 13/30 [00:33<00:43,  2.57s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  47%|████▋     | 14/30 [00:36<00:41,  2.58s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  50%|█████     | 15/30 [00:39<00:38,  2.59s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  53%|█████▎    | 16/30 [00:41<00:36,  2.57s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  57%|█████▋    | 17/30 [00:44<00:33,  2.59s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  60%|██████    | 18/30 [00:46<00:31,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  63%|██████▎   | 19/30 [00:49<00:28,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  67%|██████▋   | 20/30 [00:52<00:26,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  70%|███████   | 21/30 [00:54<00:23,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  73%|███████▎  | 22/30 [00:57<00:21,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  77%|███████▋  | 23/30 [01:00<00:18,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  80%|████████  | 24/30 [01:02<00:15,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  83%|████████▎ | 25/30 [01:05<00:13,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  87%|████████▋ | 26/30 [01:08<00:10,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  90%|█████████ | 27/30 [01:10<00:07,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  93%|█████████▎| 28/30 [01:13<00:05,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  97%|█████████▋| 29/30 [01:15<00:02,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions: 100%|██████████| 30/30 [01:18<00:00,  2.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating democrat opinion for Prompt4 run 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating opinions:   0%|          | 0/30 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:   3%|▎         | 1/30 [00:02<01:16,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:   7%|▋         | 2/30 [00:05<01:13,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  10%|█         | 3/30 [00:07<01:11,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  13%|█▎        | 4/30 [00:10<01:08,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  17%|█▋        | 5/30 [00:13<01:06,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  20%|██        | 6/30 [00:15<01:02,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  23%|██▎       | 7/30 [00:18<01:00,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  27%|██▋       | 8/30 [00:20<00:57,  2.60s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  30%|███       | 9/30 [00:23<00:54,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  33%|███▎      | 10/30 [00:26<00:52,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  37%|███▋      | 11/30 [00:28<00:49,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  40%|████      | 12/30 [00:31<00:47,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  43%|████▎     | 13/30 [00:33<00:43,  2.57s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  47%|████▋     | 14/30 [00:36<00:41,  2.58s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  50%|█████     | 15/30 [00:39<00:38,  2.59s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  53%|█████▎    | 16/30 [00:41<00:36,  2.57s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  57%|█████▋    | 17/30 [00:44<00:33,  2.60s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  60%|██████    | 18/30 [00:46<00:31,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  63%|██████▎   | 19/30 [00:49<00:28,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  67%|██████▋   | 20/30 [00:52<00:26,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  70%|███████   | 21/30 [00:54<00:23,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  73%|███████▎  | 22/30 [00:57<00:21,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  77%|███████▋  | 23/30 [01:00<00:18,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  80%|████████  | 24/30 [01:02<00:15,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  83%|████████▎ | 25/30 [01:05<00:13,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  87%|████████▋ | 26/30 [01:08<00:10,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  90%|█████████ | 27/30 [01:10<00:07,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  93%|█████████▎| 28/30 [01:13<00:05,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  97%|█████████▋| 29/30 [01:15<00:02,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions: 100%|██████████| 30/30 [01:18<00:00,  2.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating republican opinion for Prompt1 run 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating opinions:   0%|          | 0/30 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:   3%|▎         | 1/30 [00:02<01:18,  2.70s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:   7%|▋         | 2/30 [00:05<01:15,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  10%|█         | 3/30 [00:08<01:12,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  13%|█▎        | 4/30 [00:10<01:10,  2.70s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  17%|█▋        | 5/30 [00:13<01:07,  2.70s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  20%|██        | 6/30 [00:16<01:04,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  23%|██▎       | 7/30 [00:18<01:01,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  27%|██▋       | 8/30 [00:21<00:58,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  30%|███       | 9/30 [00:24<00:55,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  33%|███▎      | 10/30 [00:26<00:53,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  37%|███▋      | 11/30 [00:29<00:50,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  40%|████      | 12/30 [00:32<00:48,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  43%|████▎     | 13/30 [00:34<00:44,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  47%|████▋     | 14/30 [00:37<00:42,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  50%|█████     | 15/30 [00:39<00:39,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  53%|█████▎    | 16/30 [00:42<00:36,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  57%|█████▋    | 17/30 [00:45<00:34,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  60%|██████    | 18/30 [00:47<00:31,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  63%|██████▎   | 19/30 [00:50<00:29,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  67%|██████▋   | 20/30 [00:53<00:26,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  70%|███████   | 21/30 [00:56<00:24,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  73%|███████▎  | 22/30 [00:58<00:21,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  77%|███████▋  | 23/30 [01:01<00:18,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  80%|████████  | 24/30 [01:04<00:16,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  83%|████████▎ | 25/30 [01:06<00:13,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  87%|████████▋ | 26/30 [01:09<00:10,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  90%|█████████ | 27/30 [01:12<00:08,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  93%|█████████▎| 28/30 [01:14<00:05,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  97%|█████████▋| 29/30 [01:17<00:02,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions: 100%|██████████| 30/30 [01:20<00:00,  2.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating republican opinion for Prompt2 run 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating opinions:   0%|          | 0/30 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:   3%|▎         | 1/30 [00:02<01:17,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:   7%|▋         | 2/30 [00:05<01:14,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  10%|█         | 3/30 [00:08<01:12,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  13%|█▎        | 4/30 [00:10<01:09,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  17%|█▋        | 5/30 [00:13<01:06,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  20%|██        | 6/30 [00:15<01:03,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  23%|██▎       | 7/30 [00:18<01:00,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  27%|██▋       | 8/30 [00:21<00:57,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  30%|███       | 9/30 [00:23<00:55,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  33%|███▎      | 10/30 [00:26<00:53,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  37%|███▋      | 11/30 [00:29<00:50,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  40%|████      | 12/30 [00:31<00:47,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  43%|████▎     | 13/30 [00:34<00:44,  2.60s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  47%|████▋     | 14/30 [00:36<00:41,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  50%|█████     | 15/30 [00:39<00:39,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  53%|█████▎    | 16/30 [00:42<00:36,  2.60s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  57%|█████▋    | 17/30 [00:44<00:34,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  60%|██████    | 18/30 [00:47<00:31,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  63%|██████▎   | 19/30 [00:50<00:29,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  67%|██████▋   | 20/30 [00:52<00:26,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  70%|███████   | 21/30 [00:55<00:23,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  73%|███████▎  | 22/30 [00:58<00:21,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  77%|███████▋  | 23/30 [01:00<00:18,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  80%|████████  | 24/30 [01:03<00:15,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  83%|████████▎ | 25/30 [01:06<00:13,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  87%|████████▋ | 26/30 [01:08<00:10,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  90%|█████████ | 27/30 [01:11<00:07,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  93%|█████████▎| 28/30 [01:14<00:05,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  97%|█████████▋| 29/30 [01:16<00:02,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions: 100%|██████████| 30/30 [01:19<00:00,  2.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating republican opinion for Prompt3 run 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating opinions:   0%|          | 0/30 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:   3%|▎         | 1/30 [00:02<01:16,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:   7%|▋         | 2/30 [00:05<01:13,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  10%|█         | 3/30 [00:07<01:11,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  13%|█▎        | 4/30 [00:10<01:08,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  17%|█▋        | 5/30 [00:13<01:06,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  20%|██        | 6/30 [00:15<01:02,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  23%|██▎       | 7/30 [00:18<01:00,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  27%|██▋       | 8/30 [00:20<00:57,  2.60s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  30%|███       | 9/30 [00:23<00:54,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  33%|███▎      | 10/30 [00:26<00:52,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  37%|███▋      | 11/30 [00:28<00:49,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  40%|████      | 12/30 [00:31<00:47,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  43%|████▎     | 13/30 [00:33<00:43,  2.57s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  47%|████▋     | 14/30 [00:36<00:41,  2.58s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  50%|█████     | 15/30 [00:39<00:38,  2.59s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  53%|█████▎    | 16/30 [00:41<00:36,  2.57s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  57%|█████▋    | 17/30 [00:44<00:33,  2.59s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  60%|██████    | 18/30 [00:46<00:31,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  63%|██████▎   | 19/30 [00:49<00:28,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  67%|██████▋   | 20/30 [00:52<00:26,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  70%|███████   | 21/30 [00:54<00:23,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  73%|███████▎  | 22/30 [00:57<00:21,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  77%|███████▋  | 23/30 [01:00<00:18,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  80%|████████  | 24/30 [01:02<00:15,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  83%|████████▎ | 25/30 [01:05<00:13,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  87%|████████▋ | 26/30 [01:08<00:10,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  90%|█████████ | 27/30 [01:10<00:07,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  93%|█████████▎| 28/30 [01:13<00:05,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  97%|█████████▋| 29/30 [01:15<00:02,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions: 100%|██████████| 30/30 [01:18<00:00,  2.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating republican opinion for Prompt4 run 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating opinions:   0%|          | 0/30 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:   3%|▎         | 1/30 [00:02<01:16,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:   7%|▋         | 2/30 [00:05<01:13,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  10%|█         | 3/30 [00:07<01:11,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  13%|█▎        | 4/30 [00:10<01:08,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  17%|█▋        | 5/30 [00:13<01:06,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  20%|██        | 6/30 [00:15<01:02,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  23%|██▎       | 7/30 [00:18<01:00,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  27%|██▋       | 8/30 [00:20<00:57,  2.60s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  30%|███       | 9/30 [00:23<00:54,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  33%|███▎      | 10/30 [00:26<00:52,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  37%|███▋      | 11/30 [00:28<00:49,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  40%|████      | 12/30 [00:31<00:47,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  43%|████▎     | 13/30 [00:33<00:43,  2.57s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  47%|████▋     | 14/30 [00:36<00:41,  2.58s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  50%|█████     | 15/30 [00:39<00:38,  2.59s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  53%|█████▎    | 16/30 [00:41<00:36,  2.57s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  57%|█████▋    | 17/30 [00:44<00:33,  2.59s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  60%|██████    | 18/30 [00:46<00:31,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  63%|██████▎   | 19/30 [00:49<00:28,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  67%|██████▋   | 20/30 [00:52<00:26,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  70%|███████   | 21/30 [00:54<00:23,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  73%|███████▎  | 22/30 [00:57<00:21,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  77%|███████▋  | 23/30 [01:00<00:18,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  80%|████████  | 24/30 [01:02<00:15,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  83%|████████▎ | 25/30 [01:05<00:13,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  87%|████████▋ | 26/30 [01:08<00:10,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  90%|█████████ | 27/30 [01:10<00:07,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  93%|█████████▎| 28/30 [01:13<00:05,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  97%|█████████▋| 29/30 [01:15<00:02,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions: 100%|██████████| 30/30 [01:18<00:00,  2.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating democrat opinion for Prompt1 run 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating opinions:   0%|          | 0/30 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:   3%|▎         | 1/30 [00:02<01:18,  2.70s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:   7%|▋         | 2/30 [00:05<01:15,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  10%|█         | 3/30 [00:08<01:12,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  13%|█▎        | 4/30 [00:10<01:10,  2.70s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  17%|█▋        | 5/30 [00:13<01:07,  2.70s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  20%|██        | 6/30 [00:16<01:04,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  23%|██▎       | 7/30 [00:18<01:01,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  27%|██▋       | 8/30 [00:21<00:58,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  30%|███       | 9/30 [00:24<00:55,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  33%|███▎      | 10/30 [00:26<00:53,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  37%|███▋      | 11/30 [00:29<00:50,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  40%|████      | 12/30 [00:32<00:48,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  43%|████▎     | 13/30 [00:34<00:44,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  47%|████▋     | 14/30 [00:37<00:42,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  50%|█████     | 15/30 [00:39<00:39,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  53%|█████▎    | 16/30 [00:42<00:36,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  57%|█████▋    | 17/30 [00:45<00:34,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  60%|██████    | 18/30 [00:47<00:31,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  63%|██████▎   | 19/30 [00:50<00:29,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  67%|██████▋   | 20/30 [00:53<00:26,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  70%|███████   | 21/30 [00:56<00:24,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  73%|███████▎  | 22/30 [00:58<00:21,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  77%|███████▋  | 23/30 [01:01<00:18,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  80%|████████  | 24/30 [01:04<00:16,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  83%|████████▎ | 25/30 [01:06<00:13,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  87%|████████▋ | 26/30 [01:09<00:10,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  90%|█████████ | 27/30 [01:12<00:08,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  93%|█████████▎| 28/30 [01:14<00:05,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  97%|█████████▋| 29/30 [01:17<00:02,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions: 100%|██████████| 30/30 [01:20<00:00,  2.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating democrat opinion for Prompt2 run 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating opinions:   0%|          | 0/30 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:   3%|▎         | 1/30 [00:02<01:17,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:   7%|▋         | 2/30 [00:05<01:14,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  10%|█         | 3/30 [00:07<01:11,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  13%|█▎        | 4/30 [00:10<01:09,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  17%|█▋        | 5/30 [00:13<01:06,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  20%|██        | 6/30 [00:15<01:03,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  23%|██▎       | 7/30 [00:18<01:00,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  27%|██▋       | 8/30 [00:21<00:57,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  30%|███       | 9/30 [00:23<00:55,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  33%|███▎      | 10/30 [00:26<00:53,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  37%|███▋      | 11/30 [00:29<00:50,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  40%|████      | 12/30 [00:31<00:47,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  43%|████▎     | 13/30 [00:34<00:44,  2.59s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  47%|████▋     | 14/30 [00:36<00:41,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  50%|█████     | 15/30 [00:39<00:39,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  53%|█████▎    | 16/30 [00:42<00:36,  2.60s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  57%|█████▋    | 17/30 [00:44<00:34,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  60%|██████    | 18/30 [00:47<00:31,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  63%|██████▎   | 19/30 [00:50<00:29,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  67%|██████▋   | 20/30 [00:52<00:26,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  70%|███████   | 21/30 [00:55<00:23,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  73%|███████▎  | 22/30 [00:58<00:21,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  77%|███████▋  | 23/30 [01:00<00:18,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  80%|████████  | 24/30 [01:03<00:15,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  83%|████████▎ | 25/30 [01:06<00:13,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  87%|████████▋ | 26/30 [01:08<00:10,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  90%|█████████ | 27/30 [01:11<00:07,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  93%|█████████▎| 28/30 [01:14<00:05,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  97%|█████████▋| 29/30 [01:16<00:02,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions: 100%|██████████| 30/30 [01:19<00:00,  2.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating democrat opinion for Prompt3 run 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating opinions:   0%|          | 0/30 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:   3%|▎         | 1/30 [00:02<01:16,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:   7%|▋         | 2/30 [00:05<01:13,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  10%|█         | 3/30 [00:07<01:11,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  13%|█▎        | 4/30 [00:10<01:08,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  17%|█▋        | 5/30 [00:13<01:06,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  20%|██        | 6/30 [00:15<01:02,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  23%|██▎       | 7/30 [00:18<01:00,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  27%|██▋       | 8/30 [00:20<00:57,  2.60s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  30%|███       | 9/30 [00:23<00:54,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  33%|███▎      | 10/30 [00:26<00:52,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  37%|███▋      | 11/30 [00:28<00:49,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  40%|████      | 12/30 [00:31<00:47,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  43%|████▎     | 13/30 [00:33<00:43,  2.57s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  47%|████▋     | 14/30 [00:36<00:41,  2.58s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  50%|█████     | 15/30 [00:39<00:38,  2.59s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  53%|█████▎    | 16/30 [00:41<00:36,  2.57s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  57%|█████▋    | 17/30 [00:44<00:33,  2.60s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  60%|██████    | 18/30 [00:46<00:31,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  63%|██████▎   | 19/30 [00:49<00:28,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  67%|██████▋   | 20/30 [00:52<00:26,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  70%|███████   | 21/30 [00:54<00:23,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  73%|███████▎  | 22/30 [00:57<00:21,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  77%|███████▋  | 23/30 [01:00<00:18,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  80%|████████  | 24/30 [01:02<00:15,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  83%|████████▎ | 25/30 [01:05<00:13,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  87%|████████▋ | 26/30 [01:08<00:10,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  90%|█████████ | 27/30 [01:10<00:07,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  93%|█████████▎| 28/30 [01:13<00:05,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  97%|█████████▋| 29/30 [01:15<00:02,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions: 100%|██████████| 30/30 [01:18<00:00,  2.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating democrat opinion for Prompt4 run 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating opinions:   0%|          | 0/30 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:   3%|▎         | 1/30 [00:02<01:16,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:   7%|▋         | 2/30 [00:05<01:13,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  10%|█         | 3/30 [00:07<01:11,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  13%|█▎        | 4/30 [00:10<01:08,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  17%|█▋        | 5/30 [00:13<01:06,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  20%|██        | 6/30 [00:15<01:02,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  23%|██▎       | 7/30 [00:18<01:00,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  27%|██▋       | 8/30 [00:20<00:57,  2.60s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  30%|███       | 9/30 [00:23<00:54,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  33%|███▎      | 10/30 [00:26<00:52,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  37%|███▋      | 11/30 [00:28<00:49,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  40%|████      | 12/30 [00:31<00:47,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  43%|████▎     | 13/30 [00:33<00:43,  2.57s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  47%|████▋     | 14/30 [00:36<00:41,  2.58s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  50%|█████     | 15/30 [00:39<00:38,  2.59s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  53%|█████▎    | 16/30 [00:41<00:35,  2.57s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  57%|█████▋    | 17/30 [00:44<00:33,  2.59s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  60%|██████    | 18/30 [00:46<00:31,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  63%|██████▎   | 19/30 [00:49<00:28,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  67%|██████▋   | 20/30 [00:52<00:26,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  70%|███████   | 21/30 [00:54<00:23,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  73%|███████▎  | 22/30 [00:57<00:21,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  77%|███████▋  | 23/30 [01:00<00:18,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  80%|████████  | 24/30 [01:02<00:15,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  83%|████████▎ | 25/30 [01:05<00:13,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  87%|████████▋ | 26/30 [01:08<00:10,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  90%|█████████ | 27/30 [01:10<00:07,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  93%|█████████▎| 28/30 [01:13<00:05,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  97%|█████████▋| 29/30 [01:15<00:02,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions: 100%|██████████| 30/30 [01:18<00:00,  2.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating republican opinion for Prompt1 run 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating opinions:   0%|          | 0/30 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:   3%|▎         | 1/30 [00:02<01:18,  2.70s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:   7%|▋         | 2/30 [00:05<01:15,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  10%|█         | 3/30 [00:08<01:12,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  13%|█▎        | 4/30 [00:10<01:10,  2.70s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  17%|█▋        | 5/30 [00:13<01:08,  2.74s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  20%|██        | 6/30 [00:16<01:04,  2.70s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  23%|██▎       | 7/30 [00:18<01:01,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  27%|██▋       | 8/30 [00:21<00:58,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  30%|███       | 9/30 [00:24<00:56,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  33%|███▎      | 10/30 [00:26<00:53,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  37%|███▋      | 11/30 [00:29<00:50,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  40%|████      | 12/30 [00:32<00:48,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  43%|████▎     | 13/30 [00:34<00:44,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  47%|████▋     | 14/30 [00:37<00:42,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  50%|█████     | 15/30 [00:40<00:39,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  53%|█████▎    | 16/30 [00:42<00:36,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  57%|█████▋    | 17/30 [00:45<00:34,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  60%|██████    | 18/30 [00:48<00:31,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  63%|██████▎   | 19/30 [00:50<00:29,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  67%|██████▋   | 20/30 [00:53<00:26,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  70%|███████   | 21/30 [00:56<00:24,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  73%|███████▎  | 22/30 [00:58<00:21,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  77%|███████▋  | 23/30 [01:01<00:18,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  80%|████████  | 24/30 [01:04<00:16,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  83%|████████▎ | 25/30 [01:06<00:13,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  87%|████████▋ | 26/30 [01:09<00:10,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  90%|█████████ | 27/30 [01:12<00:08,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  93%|█████████▎| 28/30 [01:14<00:05,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  97%|█████████▋| 29/30 [01:17<00:02,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions: 100%|██████████| 30/30 [01:20<00:00,  2.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating republican opinion for Prompt2 run 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating opinions:   0%|          | 0/30 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:   3%|▎         | 1/30 [00:02<01:17,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:   7%|▋         | 2/30 [00:05<01:14,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  10%|█         | 3/30 [00:07<01:11,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  13%|█▎        | 4/30 [00:10<01:09,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  17%|█▋        | 5/30 [00:13<01:06,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  20%|██        | 6/30 [00:15<01:03,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  23%|██▎       | 7/30 [00:18<01:00,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  27%|██▋       | 8/30 [00:21<00:57,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  30%|███       | 9/30 [00:23<00:55,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  33%|███▎      | 10/30 [00:26<00:53,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  37%|███▋      | 11/30 [00:29<00:50,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  40%|████      | 12/30 [00:31<00:47,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  43%|████▎     | 13/30 [00:34<00:44,  2.60s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  47%|████▋     | 14/30 [00:36<00:41,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  50%|█████     | 15/30 [00:39<00:39,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  53%|█████▎    | 16/30 [00:42<00:36,  2.60s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  57%|█████▋    | 17/30 [00:44<00:34,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  60%|██████    | 18/30 [00:47<00:31,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  63%|██████▎   | 19/30 [00:50<00:29,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  67%|██████▋   | 20/30 [00:52<00:26,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  70%|███████   | 21/30 [00:55<00:23,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  73%|███████▎  | 22/30 [00:58<00:21,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  77%|███████▋  | 23/30 [01:00<00:18,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  80%|████████  | 24/30 [01:03<00:15,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  83%|████████▎ | 25/30 [01:06<00:13,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  87%|████████▋ | 26/30 [01:08<00:10,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  90%|█████████ | 27/30 [01:11<00:08,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  93%|█████████▎| 28/30 [01:14<00:05,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  97%|█████████▋| 29/30 [01:16<00:02,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions: 100%|██████████| 30/30 [01:19<00:00,  2.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating republican opinion for Prompt3 run 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating opinions:   0%|          | 0/30 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:   3%|▎         | 1/30 [00:02<01:16,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:   7%|▋         | 2/30 [00:05<01:13,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  10%|█         | 3/30 [00:07<01:11,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  13%|█▎        | 4/30 [00:10<01:08,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  17%|█▋        | 5/30 [00:13<01:06,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  20%|██        | 6/30 [00:15<01:02,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  23%|██▎       | 7/30 [00:18<01:00,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  27%|██▋       | 8/30 [00:20<00:57,  2.60s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  30%|███       | 9/30 [00:23<00:54,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  33%|███▎      | 10/30 [00:26<00:52,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  37%|███▋      | 11/30 [00:28<00:49,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  40%|████      | 12/30 [00:31<00:47,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  43%|████▎     | 13/30 [00:33<00:43,  2.57s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  47%|████▋     | 14/30 [00:36<00:41,  2.58s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  50%|█████     | 15/30 [00:39<00:38,  2.59s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  53%|█████▎    | 16/30 [00:41<00:36,  2.57s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  57%|█████▋    | 17/30 [00:44<00:33,  2.59s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  60%|██████    | 18/30 [00:46<00:31,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  63%|██████▎   | 19/30 [00:49<00:28,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  67%|██████▋   | 20/30 [00:52<00:26,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  70%|███████   | 21/30 [00:54<00:23,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  73%|███████▎  | 22/30 [00:57<00:21,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  77%|███████▋  | 23/30 [01:00<00:18,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  80%|████████  | 24/30 [01:02<00:15,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  83%|████████▎ | 25/30 [01:05<00:13,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  87%|████████▋ | 26/30 [01:08<00:10,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  90%|█████████ | 27/30 [01:10<00:07,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  93%|█████████▎| 28/30 [01:13<00:05,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  97%|█████████▋| 29/30 [01:15<00:02,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions: 100%|██████████| 30/30 [01:18<00:00,  2.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating republican opinion for Prompt4 run 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating opinions:   0%|          | 0/30 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:   3%|▎         | 1/30 [00:02<01:16,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:   7%|▋         | 2/30 [00:05<01:13,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  10%|█         | 3/30 [00:07<01:11,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  13%|█▎        | 4/30 [00:10<01:08,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  17%|█▋        | 5/30 [00:13<01:06,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  20%|██        | 6/30 [00:15<01:02,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  23%|██▎       | 7/30 [00:18<01:00,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  27%|██▋       | 8/30 [00:20<00:57,  2.60s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  30%|███       | 9/30 [00:23<00:54,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  33%|███▎      | 10/30 [00:26<00:52,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  37%|███▋      | 11/30 [00:28<00:49,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  40%|████      | 12/30 [00:31<00:47,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  43%|████▎     | 13/30 [00:33<00:43,  2.57s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  47%|████▋     | 14/30 [00:36<00:41,  2.58s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  50%|█████     | 15/30 [00:39<00:38,  2.59s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  53%|█████▎    | 16/30 [00:41<00:36,  2.57s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  57%|█████▋    | 17/30 [00:44<00:33,  2.59s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  60%|██████    | 18/30 [00:46<00:31,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  63%|██████▎   | 19/30 [00:49<00:28,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  67%|██████▋   | 20/30 [00:52<00:26,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  70%|███████   | 21/30 [00:54<00:23,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  73%|███████▎  | 22/30 [00:57<00:21,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  77%|███████▋  | 23/30 [01:00<00:18,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  80%|████████  | 24/30 [01:02<00:15,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  83%|████████▎ | 25/30 [01:05<00:13,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  87%|████████▋ | 26/30 [01:08<00:10,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  90%|█████████ | 27/30 [01:10<00:07,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  93%|█████████▎| 28/30 [01:13<00:05,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  97%|█████████▋| 29/30 [01:15<00:02,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions: 100%|██████████| 30/30 [01:18<00:00,  2.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating democrat opinion for Prompt1 run 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating opinions:   0%|          | 0/30 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:   3%|▎         | 1/30 [00:02<01:18,  2.70s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:   7%|▋         | 2/30 [00:05<01:15,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  10%|█         | 3/30 [00:08<01:12,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  13%|█▎        | 4/30 [00:10<01:10,  2.70s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  17%|█▋        | 5/30 [00:13<01:07,  2.70s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  20%|██        | 6/30 [00:16<01:04,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  23%|██▎       | 7/30 [00:18<01:01,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  27%|██▋       | 8/30 [00:21<00:58,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  30%|███       | 9/30 [00:24<00:55,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  33%|███▎      | 10/30 [00:26<00:53,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  37%|███▋      | 11/30 [00:29<00:50,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  40%|████      | 12/30 [00:32<00:48,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  43%|████▎     | 13/30 [00:34<00:44,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  47%|████▋     | 14/30 [00:37<00:42,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  50%|█████     | 15/30 [00:39<00:39,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  53%|█████▎    | 16/30 [00:42<00:36,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  57%|█████▋    | 17/30 [00:45<00:34,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  60%|██████    | 18/30 [00:47<00:31,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  63%|██████▎   | 19/30 [00:50<00:29,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  67%|██████▋   | 20/30 [00:53<00:26,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  70%|███████   | 21/30 [00:56<00:24,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  73%|███████▎  | 22/30 [00:58<00:21,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  77%|███████▋  | 23/30 [01:01<00:18,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  80%|████████  | 24/30 [01:04<00:16,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  83%|████████▎ | 25/30 [01:06<00:13,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  87%|████████▋ | 26/30 [01:09<00:10,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  90%|█████████ | 27/30 [01:12<00:08,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  93%|█████████▎| 28/30 [01:14<00:05,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  97%|█████████▋| 29/30 [01:17<00:02,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions: 100%|██████████| 30/30 [01:20<00:00,  2.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating democrat opinion for Prompt2 run 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating opinions:   0%|          | 0/30 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:   3%|▎         | 1/30 [00:02<01:17,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:   7%|▋         | 2/30 [00:05<01:14,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  10%|█         | 3/30 [00:08<01:12,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  13%|█▎        | 4/30 [00:10<01:09,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  17%|█▋        | 5/30 [00:13<01:06,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  20%|██        | 6/30 [00:15<01:03,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  23%|██▎       | 7/30 [00:18<01:00,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  27%|██▋       | 8/30 [00:21<00:57,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  30%|███       | 9/30 [00:23<00:55,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  33%|███▎      | 10/30 [00:26<00:53,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  37%|███▋      | 11/30 [00:29<00:50,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  40%|████      | 12/30 [00:31<00:47,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  43%|████▎     | 13/30 [00:34<00:44,  2.60s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  47%|████▋     | 14/30 [00:36<00:41,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  50%|█████     | 15/30 [00:39<00:39,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  53%|█████▎    | 16/30 [00:42<00:36,  2.60s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  57%|█████▋    | 17/30 [00:44<00:34,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  60%|██████    | 18/30 [00:47<00:31,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  63%|██████▎   | 19/30 [00:50<00:29,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  67%|██████▋   | 20/30 [00:52<00:26,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  70%|███████   | 21/30 [00:55<00:23,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  73%|███████▎  | 22/30 [00:58<00:21,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  77%|███████▋  | 23/30 [01:00<00:18,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  80%|████████  | 24/30 [01:03<00:15,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  83%|████████▎ | 25/30 [01:06<00:13,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  87%|████████▋ | 26/30 [01:08<00:10,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  90%|█████████ | 27/30 [01:11<00:08,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  93%|█████████▎| 28/30 [01:14<00:05,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  97%|█████████▋| 29/30 [01:16<00:02,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions: 100%|██████████| 30/30 [01:19<00:00,  2.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating democrat opinion for Prompt3 run 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating opinions:   0%|          | 0/30 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:   3%|▎         | 1/30 [00:02<01:16,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:   7%|▋         | 2/30 [00:05<01:13,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  10%|█         | 3/30 [00:07<01:11,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  13%|█▎        | 4/30 [00:10<01:08,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  17%|█▋        | 5/30 [00:13<01:06,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  20%|██        | 6/30 [00:15<01:02,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  23%|██▎       | 7/30 [00:18<01:00,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  27%|██▋       | 8/30 [00:20<00:57,  2.60s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  30%|███       | 9/30 [00:23<00:54,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  33%|███▎      | 10/30 [00:26<00:52,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  37%|███▋      | 11/30 [00:28<00:49,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  40%|████      | 12/30 [00:31<00:47,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  43%|████▎     | 13/30 [00:33<00:43,  2.57s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  47%|████▋     | 14/30 [00:36<00:41,  2.58s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  50%|█████     | 15/30 [00:39<00:38,  2.59s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  53%|█████▎    | 16/30 [00:41<00:35,  2.57s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  57%|█████▋    | 17/30 [00:44<00:33,  2.59s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  60%|██████    | 18/30 [00:46<00:31,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  63%|██████▎   | 19/30 [00:49<00:28,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  67%|██████▋   | 20/30 [00:52<00:26,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  70%|███████   | 21/30 [00:54<00:23,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  73%|███████▎  | 22/30 [00:57<00:21,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  77%|███████▋  | 23/30 [01:00<00:18,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  80%|████████  | 24/30 [01:02<00:15,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  83%|████████▎ | 25/30 [01:05<00:13,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  87%|████████▋ | 26/30 [01:08<00:10,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  90%|█████████ | 27/30 [01:10<00:07,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  93%|█████████▎| 28/30 [01:13<00:05,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  97%|█████████▋| 29/30 [01:15<00:02,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions: 100%|██████████| 30/30 [01:18<00:00,  2.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating democrat opinion for Prompt4 run 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating opinions:   0%|          | 0/30 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:   3%|▎         | 1/30 [00:02<01:16,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:   7%|▋         | 2/30 [00:05<01:13,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  10%|█         | 3/30 [00:07<01:11,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  13%|█▎        | 4/30 [00:10<01:08,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  17%|█▋        | 5/30 [00:13<01:06,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  20%|██        | 6/30 [00:15<01:02,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  23%|██▎       | 7/30 [00:18<01:00,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  27%|██▋       | 8/30 [00:20<00:57,  2.60s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  30%|███       | 9/30 [00:23<00:54,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  33%|███▎      | 10/30 [00:26<00:52,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  37%|███▋      | 11/30 [00:28<00:49,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  40%|████      | 12/30 [00:31<00:47,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  43%|████▎     | 13/30 [00:33<00:43,  2.57s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  47%|████▋     | 14/30 [00:36<00:41,  2.58s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  50%|█████     | 15/30 [00:39<00:38,  2.59s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  53%|█████▎    | 16/30 [00:41<00:35,  2.57s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  57%|█████▋    | 17/30 [00:44<00:33,  2.59s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  60%|██████    | 18/30 [00:46<00:31,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  63%|██████▎   | 19/30 [00:49<00:28,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  67%|██████▋   | 20/30 [00:52<00:26,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  70%|███████   | 21/30 [00:54<00:23,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  73%|███████▎  | 22/30 [00:57<00:21,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  77%|███████▋  | 23/30 [01:00<00:18,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  80%|████████  | 24/30 [01:02<00:15,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  83%|████████▎ | 25/30 [01:05<00:13,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  87%|████████▋ | 26/30 [01:08<00:10,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  90%|█████████ | 27/30 [01:10<00:07,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  93%|█████████▎| 28/30 [01:13<00:05,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  97%|█████████▋| 29/30 [01:15<00:02,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions: 100%|██████████| 30/30 [01:18<00:00,  2.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating republican opinion for Prompt1 run 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating opinions:   0%|          | 0/30 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:   3%|▎         | 1/30 [00:02<01:18,  2.70s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:   7%|▋         | 2/30 [00:05<01:15,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  10%|█         | 3/30 [00:08<01:12,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  13%|█▎        | 4/30 [00:10<01:10,  2.70s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  17%|█▋        | 5/30 [00:13<01:07,  2.70s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  20%|██        | 6/30 [00:16<01:04,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  23%|██▎       | 7/30 [00:18<01:01,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  27%|██▋       | 8/30 [00:21<00:58,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  30%|███       | 9/30 [00:24<00:55,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  33%|███▎      | 10/30 [00:26<00:53,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  37%|███▋      | 11/30 [00:29<00:50,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  40%|████      | 12/30 [00:32<00:48,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  43%|████▎     | 13/30 [00:34<00:44,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  47%|████▋     | 14/30 [00:37<00:42,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  50%|█████     | 15/30 [00:39<00:39,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  53%|█████▎    | 16/30 [00:42<00:36,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  57%|█████▋    | 17/30 [00:45<00:34,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  60%|██████    | 18/30 [00:47<00:31,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  63%|██████▎   | 19/30 [00:50<00:29,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  67%|██████▋   | 20/30 [00:53<00:26,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  70%|███████   | 21/30 [00:56<00:24,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  73%|███████▎  | 22/30 [00:58<00:21,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  77%|███████▋  | 23/30 [01:01<00:18,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  80%|████████  | 24/30 [01:04<00:16,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  83%|████████▎ | 25/30 [01:06<00:13,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  87%|████████▋ | 26/30 [01:09<00:10,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  90%|█████████ | 27/30 [01:12<00:08,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  93%|█████████▎| 28/30 [01:14<00:05,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  97%|█████████▋| 29/30 [01:17<00:02,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions: 100%|██████████| 30/30 [01:20<00:00,  2.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating republican opinion for Prompt2 run 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating opinions:   0%|          | 0/30 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:   3%|▎         | 1/30 [00:02<01:17,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:   7%|▋         | 2/30 [00:05<01:14,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  10%|█         | 3/30 [00:07<01:11,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  13%|█▎        | 4/30 [00:10<01:09,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  17%|█▋        | 5/30 [00:13<01:06,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  20%|██        | 6/30 [00:15<01:03,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  23%|██▎       | 7/30 [00:18<01:00,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  27%|██▋       | 8/30 [00:21<00:57,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  30%|███       | 9/30 [00:23<00:55,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  33%|███▎      | 10/30 [00:26<00:53,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  37%|███▋      | 11/30 [00:29<00:50,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  40%|████      | 12/30 [00:31<00:47,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  43%|████▎     | 13/30 [00:34<00:44,  2.59s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  47%|████▋     | 14/30 [00:36<00:41,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  50%|█████     | 15/30 [00:39<00:39,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  53%|█████▎    | 16/30 [00:42<00:36,  2.60s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  57%|█████▋    | 17/30 [00:44<00:34,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  60%|██████    | 18/30 [00:47<00:31,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  63%|██████▎   | 19/30 [00:50<00:29,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  67%|██████▋   | 20/30 [00:52<00:26,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  70%|███████   | 21/30 [00:55<00:23,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  73%|███████▎  | 22/30 [00:58<00:21,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  77%|███████▋  | 23/30 [01:00<00:18,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  80%|████████  | 24/30 [01:03<00:15,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  83%|████████▎ | 25/30 [01:06<00:13,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  87%|████████▋ | 26/30 [01:08<00:10,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  90%|█████████ | 27/30 [01:11<00:07,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  93%|█████████▎| 28/30 [01:14<00:05,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  97%|█████████▋| 29/30 [01:16<00:02,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions: 100%|██████████| 30/30 [01:19<00:00,  2.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating republican opinion for Prompt3 run 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating opinions:   0%|          | 0/30 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:   3%|▎         | 1/30 [00:02<01:16,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:   7%|▋         | 2/30 [00:05<01:13,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  10%|█         | 3/30 [00:07<01:11,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  13%|█▎        | 4/30 [00:10<01:08,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  17%|█▋        | 5/30 [00:13<01:06,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  20%|██        | 6/30 [00:15<01:02,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  23%|██▎       | 7/30 [00:18<01:00,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  27%|██▋       | 8/30 [00:20<00:57,  2.60s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  30%|███       | 9/30 [00:23<00:54,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  33%|███▎      | 10/30 [00:26<00:52,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  37%|███▋      | 11/30 [00:28<00:49,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  40%|████      | 12/30 [00:31<00:47,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  43%|████▎     | 13/30 [00:33<00:43,  2.57s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  47%|████▋     | 14/30 [00:36<00:41,  2.58s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  50%|█████     | 15/30 [00:39<00:38,  2.59s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  53%|█████▎    | 16/30 [00:41<00:36,  2.57s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  57%|█████▋    | 17/30 [00:44<00:33,  2.60s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  60%|██████    | 18/30 [00:46<00:31,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  63%|██████▎   | 19/30 [00:49<00:28,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  67%|██████▋   | 20/30 [00:52<00:26,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  70%|███████   | 21/30 [00:54<00:23,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  73%|███████▎  | 22/30 [00:57<00:21,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  77%|███████▋  | 23/30 [01:00<00:18,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  80%|████████  | 24/30 [01:02<00:15,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  83%|████████▎ | 25/30 [01:05<00:13,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  87%|████████▋ | 26/30 [01:08<00:10,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  90%|█████████ | 27/30 [01:10<00:07,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  93%|█████████▎| 28/30 [01:13<00:05,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  97%|█████████▋| 29/30 [01:15<00:02,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions: 100%|██████████| 30/30 [01:18<00:00,  2.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating republican opinion for Prompt4 run 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating opinions:   0%|          | 0/30 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:   3%|▎         | 1/30 [00:02<01:16,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:   7%|▋         | 2/30 [00:05<01:13,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  10%|█         | 3/30 [00:07<01:11,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  13%|█▎        | 4/30 [00:10<01:08,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  17%|█▋        | 5/30 [00:13<01:06,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  20%|██        | 6/30 [00:15<01:02,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  23%|██▎       | 7/30 [00:18<01:00,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  27%|██▋       | 8/30 [00:20<00:57,  2.60s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  30%|███       | 9/30 [00:23<00:54,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  33%|███▎      | 10/30 [00:26<00:52,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  37%|███▋      | 11/30 [00:28<00:49,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  40%|████      | 12/30 [00:31<00:47,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  43%|████▎     | 13/30 [00:33<00:43,  2.57s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  47%|████▋     | 14/30 [00:36<00:41,  2.58s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  50%|█████     | 15/30 [00:39<00:38,  2.59s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  53%|█████▎    | 16/30 [00:41<00:35,  2.57s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  57%|█████▋    | 17/30 [00:44<00:33,  2.59s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  60%|██████    | 18/30 [00:46<00:31,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  63%|██████▎   | 19/30 [00:49<00:28,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  67%|██████▋   | 20/30 [00:52<00:26,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  70%|███████   | 21/30 [00:54<00:23,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  73%|███████▎  | 22/30 [00:57<00:21,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  77%|███████▋  | 23/30 [01:00<00:18,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  80%|████████  | 24/30 [01:02<00:15,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  83%|████████▎ | 25/30 [01:05<00:13,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  87%|████████▋ | 26/30 [01:08<00:10,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  90%|█████████ | 27/30 [01:10<00:07,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  93%|█████████▎| 28/30 [01:13<00:05,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  97%|█████████▋| 29/30 [01:15<00:02,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions: 100%|██████████| 30/30 [01:18<00:00,  2.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating democrat opinion for Prompt1 run 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating opinions:   0%|          | 0/30 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:   3%|▎         | 1/30 [00:02<01:18,  2.70s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:   7%|▋         | 2/30 [00:05<01:15,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  10%|█         | 3/30 [00:08<01:12,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  13%|█▎        | 4/30 [00:10<01:10,  2.70s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  17%|█▋        | 5/30 [00:13<01:07,  2.70s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  20%|██        | 6/30 [00:16<01:04,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  23%|██▎       | 7/30 [00:18<01:01,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  27%|██▋       | 8/30 [00:21<00:58,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  30%|███       | 9/30 [00:24<00:55,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  33%|███▎      | 10/30 [00:26<00:53,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  37%|███▋      | 11/30 [00:29<00:50,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  40%|████      | 12/30 [00:32<00:48,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  43%|████▎     | 13/30 [00:34<00:44,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  47%|████▋     | 14/30 [00:37<00:42,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  50%|█████     | 15/30 [00:39<00:39,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  53%|█████▎    | 16/30 [00:42<00:36,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  57%|█████▋    | 17/30 [00:45<00:34,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  60%|██████    | 18/30 [00:47<00:31,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  63%|██████▎   | 19/30 [00:50<00:29,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  67%|██████▋   | 20/30 [00:53<00:26,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  70%|███████   | 21/30 [00:56<00:24,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  73%|███████▎  | 22/30 [00:58<00:21,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  77%|███████▋  | 23/30 [01:01<00:18,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  80%|████████  | 24/30 [01:04<00:16,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  83%|████████▎ | 25/30 [01:06<00:13,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  87%|████████▋ | 26/30 [01:09<00:10,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  90%|█████████ | 27/30 [01:12<00:08,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  93%|█████████▎| 28/30 [01:14<00:05,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  97%|█████████▋| 29/30 [01:17<00:02,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions: 100%|██████████| 30/30 [01:20<00:00,  2.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating democrat opinion for Prompt2 run 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating opinions:   0%|          | 0/30 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:   3%|▎         | 1/30 [00:02<01:17,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:   7%|▋         | 2/30 [00:05<01:14,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  10%|█         | 3/30 [00:08<01:12,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  13%|█▎        | 4/30 [00:10<01:09,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  17%|█▋        | 5/30 [00:13<01:06,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  20%|██        | 6/30 [00:15<01:03,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  23%|██▎       | 7/30 [00:18<01:00,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  27%|██▋       | 8/30 [00:21<00:57,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  30%|███       | 9/30 [00:23<00:55,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  33%|███▎      | 10/30 [00:26<00:53,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  37%|███▋      | 11/30 [00:29<00:50,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  40%|████      | 12/30 [00:31<00:47,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  43%|████▎     | 13/30 [00:34<00:44,  2.59s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  47%|████▋     | 14/30 [00:36<00:41,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  50%|█████     | 15/30 [00:39<00:39,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  53%|█████▎    | 16/30 [00:42<00:36,  2.60s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  57%|█████▋    | 17/30 [00:44<00:34,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  60%|██████    | 18/30 [00:47<00:31,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  63%|██████▎   | 19/30 [00:50<00:29,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  67%|██████▋   | 20/30 [00:52<00:26,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  70%|███████   | 21/30 [00:55<00:23,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  73%|███████▎  | 22/30 [00:58<00:21,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  77%|███████▋  | 23/30 [01:00<00:18,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  80%|████████  | 24/30 [01:03<00:15,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  83%|████████▎ | 25/30 [01:06<00:13,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  87%|████████▋ | 26/30 [01:08<00:10,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  90%|█████████ | 27/30 [01:11<00:07,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  93%|█████████▎| 28/30 [01:14<00:05,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  97%|█████████▋| 29/30 [01:16<00:02,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions: 100%|██████████| 30/30 [01:19<00:00,  2.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating democrat opinion for Prompt3 run 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating opinions:   0%|          | 0/30 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:   3%|▎         | 1/30 [00:02<01:16,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:   7%|▋         | 2/30 [00:05<01:13,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  10%|█         | 3/30 [00:07<01:11,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  13%|█▎        | 4/30 [00:10<01:08,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  17%|█▋        | 5/30 [00:13<01:06,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  20%|██        | 6/30 [00:15<01:02,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  23%|██▎       | 7/30 [00:18<01:00,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  27%|██▋       | 8/30 [00:20<00:57,  2.60s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  30%|███       | 9/30 [00:23<00:54,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  33%|███▎      | 10/30 [00:26<00:52,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  37%|███▋      | 11/30 [00:28<00:49,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  40%|████      | 12/30 [00:31<00:47,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  43%|████▎     | 13/30 [00:33<00:43,  2.57s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  47%|████▋     | 14/30 [00:36<00:41,  2.58s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  50%|█████     | 15/30 [00:39<00:38,  2.59s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  53%|█████▎    | 16/30 [00:41<00:35,  2.57s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  57%|█████▋    | 17/30 [00:44<00:33,  2.59s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  60%|██████    | 18/30 [00:46<00:31,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  63%|██████▎   | 19/30 [00:49<00:28,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  67%|██████▋   | 20/30 [00:52<00:26,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  70%|███████   | 21/30 [00:54<00:23,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  73%|███████▎  | 22/30 [00:57<00:21,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  77%|███████▋  | 23/30 [01:00<00:18,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  80%|████████  | 24/30 [01:02<00:15,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  83%|████████▎ | 25/30 [01:05<00:13,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  87%|████████▋ | 26/30 [01:08<00:10,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  90%|█████████ | 27/30 [01:10<00:07,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  93%|█████████▎| 28/30 [01:13<00:05,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  97%|█████████▋| 29/30 [01:15<00:02,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions: 100%|██████████| 30/30 [01:18<00:00,  2.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating democrat opinion for Prompt4 run 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating opinions:   0%|          | 0/30 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:   3%|▎         | 1/30 [00:02<01:16,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:   7%|▋         | 2/30 [00:05<01:13,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  10%|█         | 3/30 [00:07<01:11,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  13%|█▎        | 4/30 [00:10<01:08,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  17%|█▋        | 5/30 [00:13<01:06,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  20%|██        | 6/30 [00:15<01:02,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  23%|██▎       | 7/30 [00:18<01:00,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  27%|██▋       | 8/30 [00:20<00:57,  2.60s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  30%|███       | 9/30 [00:23<00:54,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  33%|███▎      | 10/30 [00:26<00:52,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  37%|███▋      | 11/30 [00:28<00:49,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  40%|████      | 12/30 [00:31<00:47,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  43%|████▎     | 13/30 [00:33<00:43,  2.57s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  47%|████▋     | 14/30 [00:36<00:41,  2.58s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  50%|█████     | 15/30 [00:39<00:38,  2.59s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  53%|█████▎    | 16/30 [00:41<00:36,  2.57s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  57%|█████▋    | 17/30 [00:44<00:33,  2.60s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  60%|██████    | 18/30 [00:46<00:31,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  63%|██████▎   | 19/30 [00:49<00:28,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  67%|██████▋   | 20/30 [00:52<00:26,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  70%|███████   | 21/30 [00:54<00:23,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  73%|███████▎  | 22/30 [00:57<00:21,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  77%|███████▋  | 23/30 [01:00<00:18,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  80%|████████  | 24/30 [01:02<00:15,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  83%|████████▎ | 25/30 [01:05<00:13,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  87%|████████▋ | 26/30 [01:08<00:10,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  90%|█████████ | 27/30 [01:10<00:07,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  93%|█████████▎| 28/30 [01:13<00:05,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  97%|█████████▋| 29/30 [01:15<00:02,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions: 100%|██████████| 30/30 [01:18<00:00,  2.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating republican opinion for Prompt1 run 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating opinions:   0%|          | 0/30 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:   3%|▎         | 1/30 [00:02<01:18,  2.70s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:   7%|▋         | 2/30 [00:05<01:15,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  10%|█         | 3/30 [00:08<01:12,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  13%|█▎        | 4/30 [00:10<01:10,  2.70s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  17%|█▋        | 5/30 [00:13<01:07,  2.70s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  20%|██        | 6/30 [00:16<01:04,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  23%|██▎       | 7/30 [00:18<01:01,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  27%|██▋       | 8/30 [00:21<00:58,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  30%|███       | 9/30 [00:24<00:55,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  33%|███▎      | 10/30 [00:26<00:53,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  37%|███▋      | 11/30 [00:29<00:50,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating opinions:  37%|███▋      | 11/30 [00:30<00:52,  2.77s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerating \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparty\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m opinion for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt_option\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m run \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m output_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput/CommunityLM_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparty\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-twitter-gpt2/run_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 10\u001b[0m \u001b[43mgenerate_community_opinion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_option\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/llments/examples/community_lm/community_lm_utils.py:49\u001b[0m, in \u001b[0;36mgenerate_community_opinion\u001b[0;34m(model, prompt_option, output_path, seed, preceding_prompt, overwrite)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     46\u001b[0m total_prompt \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([preceding_prompt, prompt]) \u001b[38;5;28;01mif\u001b[39;00m preceding_prompt \u001b[38;5;28;01melse\u001b[39;00m prompt\n\u001b[1;32m     48\u001b[0m )\n\u001b[0;32m---> 49\u001b[0m responses \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtotal_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m responses \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m responses]\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(output_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m out:\n",
      "File \u001b[0;32m~/llments/llments/lm/base/hugging_face.py:98\u001b[0m, in \u001b[0;36mHuggingFaceLM.generate\u001b[0;34m(self, condition, do_sample, max_length, max_new_tokens, temperature, num_return_sequences)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Generate an output given the language model.\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \n\u001b[1;32m     78\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;124;03m    str: A sampled output sequence from the language model.\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     92\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(\n\u001b[1;32m     93\u001b[0m     condition,\n\u001b[1;32m     94\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     95\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mmax_length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     96\u001b[0m     max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[1;32m     97\u001b[0m )\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 98\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_return_sequences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_sample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mdecode(output, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs\n\u001b[1;32m    109\u001b[0m ]\n",
      "File \u001b[0;32m~/miniconda3/envs/capstone/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/capstone/lib/python3.10/site-packages/transformers/generation/utils.py:2215\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2207\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2208\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2209\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2210\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2211\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2212\u001b[0m     )\n\u001b[1;32m   2214\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2215\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2216\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2217\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2218\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2219\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2220\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2221\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2222\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2223\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2225\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2226\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2227\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2228\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2229\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2234\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2235\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/capstone/lib/python3.10/site-packages/transformers/generation/utils.py:3206\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3203\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[1;32m   3205\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 3206\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3208\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[1;32m   3209\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   3210\u001b[0m     outputs,\n\u001b[1;32m   3211\u001b[0m     model_kwargs,\n\u001b[1;32m   3212\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   3213\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/capstone/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/capstone/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/capstone/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:1271\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1263\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1264\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1265\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1266\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1267\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1268\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1269\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1271\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1275\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1276\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1277\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1278\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1279\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1280\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1281\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1282\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1286\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1288\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/capstone/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/capstone/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/capstone/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:1132\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1121\u001b[0m         block\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1122\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1129\u001b[0m         output_attentions,\n\u001b[1;32m   1130\u001b[0m     )\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1132\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1133\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1134\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1135\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1137\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1138\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1139\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1140\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1141\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1143\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/capstone/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/capstone/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/capstone/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:652\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    650\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    651\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_2(hidden_states)\n\u001b[0;32m--> 652\u001b[0m feed_forward_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[38;5;66;03m# residual connection\u001b[39;00m\n\u001b[1;32m    654\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m feed_forward_hidden_states\n",
      "File \u001b[0;32m~/miniconda3/envs/capstone/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/capstone/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/capstone/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:576\u001b[0m, in \u001b[0;36mGPT2MLP.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: Optional[Tuple[torch\u001b[38;5;241m.\u001b[39mFloatTensor]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor:\n\u001b[1;32m    575\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_fc(hidden_states)\n\u001b[0;32m--> 576\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    577\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_proj(hidden_states)\n\u001b[1;32m    578\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n",
      "File \u001b[0;32m~/miniconda3/envs/capstone/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/capstone/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/capstone/lib/python3.10/site-packages/transformers/activations.py:56\u001b[0m, in \u001b[0;36mNewGELUActivation.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtanh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.044715\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3.0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for run in range(1, 6):\n",
    "    for party in ['democrat', 'republican']:\n",
    "        # This uses the pre-trained communitylm, but you can uncomment if you trained your own model\n",
    "        lm_name = f'CommunityLM/{party}-twitter-gpt2'\n",
    "        # lm_name = f'./data/{party}-twitter-gpt2'\n",
    "        lm = HuggingFaceLM(lm_name, device=device)\n",
    "        for prompt_option in ['Prompt1', 'Prompt2', 'Prompt3', 'Prompt4']:\n",
    "            print(f'generating {party} opinion for {prompt_option} run {run}...')\n",
    "            output_path = f'output/CommunityLM_{party}-twitter-gpt2/run_{run}'\n",
    "            generate_community_opinion(lm, prompt_option, output_path, run)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348fc5e7-aad4-4d1a-9436-0ae83585e8bb",
   "metadata": {},
   "source": [
    "## Perform Group-level Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2049390",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = HuggingFaceSentimentEvaluator(\n",
    "    \"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
    "    device=device\n",
    ")\n",
    "for party in ['democrat', 'republican']:\n",
    "    compute_group_stance(\n",
    "        evaluator=evaluator,\n",
    "        data_folder=f'output/CommunityLM_{party}-twitter-gpt2',\n",
    "        output_filename=f'output/CommunityLM_{party}-twitter-gpt2/stance_prediction.csv',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec53be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dem = pd.read_csv(\"output/CommunityLM_democrat-twitter-gpt2/stance_prediction.csv\")\n",
    "df_repub = pd.read_csv(\"output/CommunityLM_republican-twitter-gpt2/stance_prediction.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6017a1d8-ae02-4adb-b3af-3d19911a61a2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Preparing ANES2020 Questions\n",
    "\n",
    "This is data from the American National Election Study (ANES)\n",
    "\n",
    "Website: https://electionstudies.org/\n",
    "Email:   anes@electionstudies.org\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e5cf0c-3f2c-4cae-806a-3798f8138664",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/anes_pilot_2020ets_csv.csv\")\n",
    "\n",
    "print(f\"Number of Rows Total {df.shape}\")\n",
    "\n",
    "# only look self identified partisans 2144/3080. 1: Republican; 2: Democrat\n",
    "df = df[df.pid1r < 3]\n",
    "df.pid1r = df.pid1r.map({1: \"Republican\", 2: \"Democrat\"})\n",
    "print(f\"Number of Rows for Partisans {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976e4ba7-6c58-4445-9522-fe844342df1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 999 stands for missing values and 'pid1r' is the partisanship\n",
    "df_politician_results = df[['pid1r']+politician_feelings+groups_feelings].replace(999, np.nan).groupby(\"pid1r\").mean().T\n",
    "df_politician_results['is_repub_leading'] = (df_politician_results.Republican > df_politician_results.Democrat)\n",
    "# df_politician_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34e06de-bfdd-4475-a4d6-47a17d627bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_politician_results['Prompt1'] = anes_df['Prompt1'].to_list()\n",
    "df_politician_results['Prompt2'] = anes_df['Prompt2'].to_list()\n",
    "df_politician_results['Prompt3'] = anes_df['Prompt3'].to_list()\n",
    "df_politician_results['Prompt4'] = anes_df['Prompt4'].to_list()\n",
    "\n",
    "df_politician_results['pid'] = df_politician_results.index\n",
    "df_politician_results.to_csv(\"output/anes2020_pilot_prompt_probing.csv\", index=False)\n",
    "# df_politician_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabcbbde-38a0-4e7c-a0a3-93034ce589c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_politician_results['diff'] = (df_politician_results.Democrat-df_politician_results.Republican).apply(abs)\n",
    "df_politician_results.sort_values(by=['diff'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fe8318-5836-448c-bb50-301845732f53",
   "metadata": {},
   "source": [
    "## Evaluate fine-tuned GPT-2 CommunityLM models\n",
    "\n",
    "This evaluates the sentiment of the completions generated by each model according to a sentiment classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f6811b-099e-4ba5-993c-3b7ada968f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "def compute_scores(df_anes, df_dem, df_repub):\n",
    "    df_repub['prediction'] = (df_repub['group_sentiment'] > df_dem['group_sentiment'])\n",
    "\n",
    "    gold_labels = df_anes.is_repub_leading.astype(int).values\n",
    "    rows = []\n",
    "    for run in range(1, 6):\n",
    "        run = \"run_{}\".format(run)\n",
    "        for prompt_format in range(1, 5):\n",
    "            prompt_format = \"Prompt{}\".format(prompt_format)\n",
    "            df_ = df_repub[(df_repub.run == run) & (df_repub.prompt_format == prompt_format)]\n",
    "            pred_labels = df_.prediction.astype(int).values\n",
    "            acc = accuracy_score(gold_labels, pred_labels) \n",
    "            p, r, f1, _ = precision_recall_fscore_support(gold_labels, pred_labels, average='weighted')\n",
    "            rows.append([run, prompt_format, acc, p, r, f1])\n",
    "    df_scores = pd.DataFrame(rows, columns=[\"run\", \"prompt_format\", \"accuracy\", \"precision\", \"recall\", \"f1\"])\n",
    "    return df_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d429b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_repub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a6ef2b-ff35-49dc-92a7-0fb984fed6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"output/anes2020_pilot_prompt_probing.csv\")\n",
    "df_scores = compute_scores(df, df_dem, df_repub)\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200fb627-57f4-4d73-a375-bb87e95923c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract gold ranks\n",
    "df_politician_results = df_politician_results.sort_values(by=[\"pid\"])\n",
    "gold_dem_rank = df_politician_results['Democrat'].rank().values\n",
    "gold_repub_rank = df_politician_results['Republican'].rank().values\n",
    "gold_repub_rank\n",
    "\n",
    "from scipy import stats\n",
    "def extract_ranking(df_):\n",
    "    df_ = df_.sort_values(by=['question'])\n",
    "    return df_[df_.prompt_format == \"Prompt4\"].groupby(['question']).group_sentiment.mean().rank().values\n",
    "\n",
    "dem_rank = extract_ranking(df_dem)\n",
    "repub_rank = extract_ranking(df_repub)\n",
    "\n",
    "gold_dem_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69f6bc3-b8d2-44ad-9aab-222653191c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot the rankings\n",
    "\n",
    "def extract_ranking_for_politicians(df_):\n",
    "    df_ = df_[df_.question.isin(politician_feelings)]\n",
    "    df_ = df_.sort_values(by=['question', 'run'])\n",
    "    return df_[df_.prompt_format == \"Prompt4\"]\n",
    "\n",
    "df_politician_results = df_politician_results[df_politician_results.pid.isin(politician_feelings)].sort_values(by=['pid'])\n",
    "df_politician_results['short_name'] = df_politician_results.Prompt1.apply(lambda x: x.split(\" \")[-1])\n",
    "\n",
    "dem_politician_rank = extract_ranking_for_politicians(df_dem)\n",
    "df_avg = dem_politician_rank.groupby(\"question\").group_sentiment.mean().reset_index()\n",
    "df_avg['group_avg_sentiment'] = df_avg['group_sentiment']\n",
    "del df_avg[\"group_sentiment\"]\n",
    "dem_politician_rank = dem_politician_rank.merge(df_politician_results, left_on=\"question\", right_on=\"pid\")\n",
    "dem_politician_rank = dem_politician_rank.merge(df_avg, on=\"question\")\n",
    "\n",
    "\n",
    "repub_politician_rank = extract_ranking_for_politicians(df_repub)\n",
    "df_avg = repub_politician_rank.groupby(\"question\").group_sentiment.mean().reset_index()\n",
    "df_avg['group_avg_sentiment'] = df_avg['group_sentiment']\n",
    "del df_avg[\"group_sentiment\"]\n",
    "repub_politician_rank = repub_politician_rank.merge(df_politician_results, left_on=\"question\", right_on=\"pid\")\n",
    "repub_politician_rank = repub_politician_rank.merge(df_avg, on=\"question\")\n",
    "\n",
    "\n",
    "dem_politician_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015bb056-a742-49a1-97a7-dda18d203ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_politician_results.to_csv(\"rank_plots.csv\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(rc={'figure.figsize':(5,5)})\n",
    "\n",
    "palette = sns.color_palette(\"Blues\",n_colors=20)\n",
    "palette.reverse()\n",
    "\n",
    "ax = sns.barplot(data=dem_politician_rank.sort_values(by=\"group_avg_sentiment\", ascending=False), x=\"group_sentiment\", y=\"short_name\", palette=palette, estimator=np.mean, ci=90)\n",
    "\n",
    "ax.set_xlabel(None, fontsize=15)\n",
    "ax.set_ylabel(None)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('rankings/finetuned_gpt2_pred_dem_rank.png', bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3b9c6c-a2e1-43b4-8463-caaff9653fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(5,5)})\n",
    "\n",
    "palette = sns.color_palette(\"Blues\",n_colors=20)\n",
    "palette.reverse()\n",
    "\n",
    "ax = sns.barplot(data=dem_politician_rank.sort_values(by=\"Democrat\", ascending=False), x=\"Democrat\", y=\"short_name\", palette=palette)\n",
    "\n",
    "ax.set_xlabel(None, fontsize=15)\n",
    "ax.set_ylabel(None)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('rankings/gold_dem_rank.png', bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc980b7-aa69-4cbe-8778-f57b463fc909",
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = sns.color_palette(\"Reds\", n_colors=20)\n",
    "palette.reverse()\n",
    "\n",
    "ax = sns.barplot(data=repub_politician_rank.sort_values(by=\"group_avg_sentiment\", ascending=False), x=\"group_sentiment\", y=\"short_name\", palette=palette)\n",
    "\n",
    "ax.set_xlabel(None, fontsize=15)\n",
    "ax.set_ylabel(None)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('rankings/finetuned_gpt2_pred_repub_rank.png', bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1dd3c2-dda4-482e-99b6-6ef4316155ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = sns.color_palette(\"Reds\", n_colors=20)\n",
    "palette.reverse()\n",
    "\n",
    "ax = sns.barplot(data=repub_politician_rank.sort_values(by=\"Republican\", ascending=False), x=\"Republican\", y=\"short_name\", palette=palette)\n",
    "\n",
    "ax.set_xlabel(None, fontsize=15)\n",
    "ax.set_ylabel(None)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('rankings/gold_repub_rank.png', bbox_inches = \"tight\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (capstone)",
   "language": "python",
   "name": "capstone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
