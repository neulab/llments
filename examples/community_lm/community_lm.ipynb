{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ca8370c",
   "metadata": {},
   "source": [
    "# CommunityLM\n",
    "\n",
    "This is a replication of the experiments from [CommunityLM](https://arxiv.org/abs/2209.07065) (Jiang et al. 2022), which probes partisan worldviews from language models, based on the [original repo](https://github.com/hjian42/communitylm).\n",
    "\n",
    "Running all the experiments on a single GPU takes about 3-4 hours.\n",
    "\n",
    "Before running the notebook, please install requirements and download the data.\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "bash download_data.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29563e5d-41b0-4f89-8d8b-a54b40f8dfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llments.lm.base.hugging_face import HuggingFaceLM, HuggingFaceLMFitter\n",
    "from llments.lm.base.empirical import load_from_text_file\n",
    "from llments.eval.sentiment import HuggingFaceSentimentEvaluator\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from community_lm_constants import politician_feelings, groups_feelings, anes_df\n",
    "from community_lm_utils import generate_community_opinion, compute_group_stance\n",
    "\n",
    "device = 'mps'  # change to 'mps' if you have a mac, or 'cuda:0' if you have an NVIDIA GPU "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4683f755",
   "metadata": {},
   "source": [
    "## Train a CommunityLM model (optional)\n",
    "\n",
    "The CommunityLM paper has released their pre-trained models on Hugging Face, so for the purpose of this notebook, we will use the pre-trained models. However, if you want to train a CommunityLM model from scratch, you can download training data to `data/{democrat,republican}-tweets.txt`, uncomment the following lines, and replace the `lm_name` variable in the following cell with `./data/{party}-twitter-gpt2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbd430fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [01:41<00:00,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 101.0505, 'train_samples_per_second': 15.834, 'train_steps_per_second': 1.979, 'train_loss': 4.220817260742187, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [01:25<00:00,  2.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 85.1434, 'train_samples_per_second': 18.792, 'train_steps_per_second': 2.349, 'train_loss': 4.143599548339844, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "base_model = HuggingFaceLM(\"gpt2\", device=device)\n",
    "for party in ['democrat', 'republican']:\n",
    "    dataset = load_from_text_file(f\"data/{party}-tweets.txt\")\n",
    "    fit_model = HuggingFaceLMFitter.fit(base_model, dataset, output_dir=f\"data/{party}-party-gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0022efe",
   "metadata": {},
   "source": [
    "## Generate Opinions using CommunityLM\n",
    "\n",
    "The following code generates opinions using CommunityLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bacd15ad",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Incorrect path_or_model_id: './data/democrat-twitter-gpt2'. Please provide either the path to a local folder or the repo_id of a model on the Hub.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/llments/lib/python3.11/site-packages/transformers/utils/hub.py:398\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 398\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/llments/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:110\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 110\u001b[0m     \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m arg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/llments/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:158\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m repo_id\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 158\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must be in the form \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrepo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnamespace/repo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Use `repo_type` argument if needed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX\u001b[38;5;241m.\u001b[39mmatch(repo_id):\n",
      "\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': './data/democrat-twitter-gpt2'. Use `repo_type` argument if needed.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m party \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdemocrat\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrepublican\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# lm_name = f'CommunityLM/{party}-twitter-gpt2'\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     lm_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparty\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-twitter-gpt2\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 5\u001b[0m     lm \u001b[38;5;241m=\u001b[39m \u001b[43mHuggingFaceLM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlm_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m prompt_option \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrompt1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrompt2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrompt3\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrompt4\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerating \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparty\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m opinion for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt_option\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m run \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/llments/lib/python3.11/site-packages/llments/lm/base/hugging_face.py:36\u001b[0m, in \u001b[0;36mHuggingFaceLM.__init__\u001b[0;34m(self, model, tokenizer_path, device, cache_dir)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     32\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou need to install the `transformers` package to use this class.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     33\u001b[0m     )\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.ckpt\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model:  \u001b[38;5;66;03m# use the same tokenizer as the model\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     40\u001b[0m         model, do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, use_cache\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, cache_dir\u001b[38;5;241m=\u001b[39mcache_dir\n\u001b[1;32m     41\u001b[0m     )\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m device \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/llments/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:767\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    766\u001b[0m \u001b[38;5;66;03m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[39;00m\n\u001b[0;32m--> 767\u001b[0m tokenizer_config \u001b[38;5;241m=\u001b[39m \u001b[43mget_tokenizer_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m tokenizer_config:\n\u001b[1;32m    769\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m tokenizer_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/llments/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:600\u001b[0m, in \u001b[0;36mget_tokenizer_config\u001b[0;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, **kwargs)\u001b[0m\n\u001b[1;32m    597\u001b[0m     token \u001b[38;5;241m=\u001b[39m use_auth_token\n\u001b[1;32m    599\u001b[0m commit_hash \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 600\u001b[0m resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m    \u001b[49m\u001b[43mTOKENIZER_CONFIG_FILE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_gated_repo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    617\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not locate the tokenizer configuration file, will try to use the model config instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/llments/lib/python3.11/site-packages/transformers/utils/hub.py:462\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere was a specific connection error when trying to load \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HFValidationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 462\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    463\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncorrect path_or_model_id: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Please provide either the path to a local folder or the repo_id of a model on the Hub.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    464\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resolved_file\n",
      "\u001b[0;31mOSError\u001b[0m: Incorrect path_or_model_id: './data/democrat-twitter-gpt2'. Please provide either the path to a local folder or the repo_id of a model on the Hub."
     ]
    }
   ],
   "source": [
    "for run in range(1, 6):\n",
    "    for party in ['democrat', 'republican']:\n",
    "        # lm_name = f'CommunityLM/{party}-twitter-gpt2'\n",
    "        lm_name = f'./data/{party}-twitter-gpt2'\n",
    "        lm = HuggingFaceLM(lm_name, device=device)\n",
    "        for prompt_option in ['Prompt1', 'Prompt2', 'Prompt3', 'Prompt4']:\n",
    "            print(f'generating {party} opinion for {prompt_option} run {run}...')\n",
    "            output_path = f'output/CommunityLM_{party}-twitter-gpt2/run_{run}'\n",
    "            generate_community_opinion(lm, prompt_option, output_path, run)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348fc5e7-aad4-4d1a-9436-0ae83585e8bb",
   "metadata": {},
   "source": [
    "## Perform Group-level Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2049390",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = HuggingFaceSentimentEvaluator(\n",
    "    \"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
    "    device=device\n",
    ")\n",
    "for party in ['democrat', 'republican']:\n",
    "    compute_group_stance(\n",
    "        evaluator=evaluator,\n",
    "        data_folder=f'output/CommunityLM_{party}-twitter-gpt2',\n",
    "        output_filename=f'output/CommunityLM_{party}-twitter-gpt2/stance_prediction.csv',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec53be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dem = pd.read_csv(\"output/CommunityLM_democrat-twitter-gpt2/stance_prediction.csv\")\n",
    "df_repub = pd.read_csv(\"output/CommunityLM_republican-twitter-gpt2/stance_prediction.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6017a1d8-ae02-4adb-b3af-3d19911a61a2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Preparing ANES2020 Questions\n",
    "\n",
    "This is data from the American National Election Study (ANES)\n",
    "\n",
    "Website: https://electionstudies.org/\n",
    "Email:   anes@electionstudies.org\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e5cf0c-3f2c-4cae-806a-3798f8138664",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/anes_pilot_2020ets_csv.csv\")\n",
    "\n",
    "print(f\"Number of Rows Total {df.shape}\")\n",
    "\n",
    "# only look self identified partisans 2144/3080. 1: Republican; 2: Democrat\n",
    "df = df[df.pid1r < 3]\n",
    "df.pid1r = df.pid1r.map({1: \"Republican\", 2: \"Democrat\"})\n",
    "print(f\"Number of Rows for Partisans {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976e4ba7-6c58-4445-9522-fe844342df1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 999 stands for missing values and 'pid1r' is the partisanship\n",
    "df_politician_results = df[['pid1r']+politician_feelings+groups_feelings].replace(999, np.nan).groupby(\"pid1r\").mean().T\n",
    "df_politician_results['is_repub_leading'] = (df_politician_results.Republican > df_politician_results.Democrat)\n",
    "# df_politician_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34e06de-bfdd-4475-a4d6-47a17d627bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_politician_results['Prompt1'] = anes_df['Prompt1'].to_list()\n",
    "df_politician_results['Prompt2'] = anes_df['Prompt2'].to_list()\n",
    "df_politician_results['Prompt3'] = anes_df['Prompt3'].to_list()\n",
    "df_politician_results['Prompt4'] = anes_df['Prompt4'].to_list()\n",
    "\n",
    "df_politician_results['pid'] = df_politician_results.index\n",
    "df_politician_results.to_csv(\"output/anes2020_pilot_prompt_probing.csv\", index=False)\n",
    "# df_politician_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabcbbde-38a0-4e7c-a0a3-93034ce589c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_politician_results['diff'] = (df_politician_results.Democrat-df_politician_results.Republican).apply(abs)\n",
    "df_politician_results.sort_values(by=['diff'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fe8318-5836-448c-bb50-301845732f53",
   "metadata": {},
   "source": [
    "## Evaluate fine-tuned GPT-2 CommunityLM models\n",
    "\n",
    "This evaluates the sentiment of the completions generated by each model according to a sentiment classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f6811b-099e-4ba5-993c-3b7ada968f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "def compute_scores(df_anes, df_dem, df_repub):\n",
    "    df_repub['prediction'] = (df_repub['group_sentiment'] > df_dem['group_sentiment'])\n",
    "\n",
    "    gold_labels = df_anes.is_repub_leading.astype(int).values\n",
    "    rows = []\n",
    "    for run in range(1, 6):\n",
    "        run = \"run_{}\".format(run)\n",
    "        for prompt_format in range(1, 5):\n",
    "            prompt_format = \"Prompt{}\".format(prompt_format)\n",
    "            df_ = df_repub[(df_repub.run == run) & (df_repub.prompt_format == prompt_format)]\n",
    "            pred_labels = df_.prediction.astype(int).values\n",
    "            acc = accuracy_score(gold_labels, pred_labels) \n",
    "            p, r, f1, _ = precision_recall_fscore_support(gold_labels, pred_labels, average='weighted')\n",
    "            rows.append([run, prompt_format, acc, p, r, f1])\n",
    "    df_scores = pd.DataFrame(rows, columns=[\"run\", \"prompt_format\", \"accuracy\", \"precision\", \"recall\", \"f1\"])\n",
    "    return df_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d429b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_repub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a6ef2b-ff35-49dc-92a7-0fb984fed6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"output/anes2020_pilot_prompt_probing.csv\")\n",
    "df_scores = compute_scores(df, df_dem, df_repub)\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200fb627-57f4-4d73-a375-bb87e95923c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract gold ranks\n",
    "df_politician_results = df_politician_results.sort_values(by=[\"pid\"])\n",
    "gold_dem_rank = df_politician_results['Democrat'].rank().values\n",
    "gold_repub_rank = df_politician_results['Republican'].rank().values\n",
    "gold_repub_rank\n",
    "\n",
    "from scipy import stats\n",
    "def extract_ranking(df_):\n",
    "    df_ = df_.sort_values(by=['question'])\n",
    "    return df_[df_.prompt_format == \"Prompt4\"].groupby(['question']).group_sentiment.mean().rank().values\n",
    "\n",
    "dem_rank = extract_ranking(df_dem)\n",
    "repub_rank = extract_ranking(df_repub)\n",
    "\n",
    "gold_dem_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69f6bc3-b8d2-44ad-9aab-222653191c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot the rankings\n",
    "\n",
    "def extract_ranking_for_politicians(df_):\n",
    "    df_ = df_[df_.question.isin(politician_feelings)]\n",
    "    df_ = df_.sort_values(by=['question', 'run'])\n",
    "    return df_[df_.prompt_format == \"Prompt4\"]\n",
    "\n",
    "df_politician_results = df_politician_results[df_politician_results.pid.isin(politician_feelings)].sort_values(by=['pid'])\n",
    "df_politician_results['short_name'] = df_politician_results.Prompt1.apply(lambda x: x.split(\" \")[-1])\n",
    "\n",
    "dem_politician_rank = extract_ranking_for_politicians(df_dem)\n",
    "df_avg = dem_politician_rank.groupby(\"question\").group_sentiment.mean().reset_index()\n",
    "df_avg['group_avg_sentiment'] = df_avg['group_sentiment']\n",
    "del df_avg[\"group_sentiment\"]\n",
    "dem_politician_rank = dem_politician_rank.merge(df_politician_results, left_on=\"question\", right_on=\"pid\")\n",
    "dem_politician_rank = dem_politician_rank.merge(df_avg, on=\"question\")\n",
    "\n",
    "\n",
    "repub_politician_rank = extract_ranking_for_politicians(df_repub)\n",
    "df_avg = repub_politician_rank.groupby(\"question\").group_sentiment.mean().reset_index()\n",
    "df_avg['group_avg_sentiment'] = df_avg['group_sentiment']\n",
    "del df_avg[\"group_sentiment\"]\n",
    "repub_politician_rank = repub_politician_rank.merge(df_politician_results, left_on=\"question\", right_on=\"pid\")\n",
    "repub_politician_rank = repub_politician_rank.merge(df_avg, on=\"question\")\n",
    "\n",
    "\n",
    "dem_politician_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015bb056-a742-49a1-97a7-dda18d203ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_politician_results.to_csv(\"rank_plots.csv\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(rc={'figure.figsize':(5,5)})\n",
    "\n",
    "palette = sns.color_palette(\"Blues\",n_colors=20)\n",
    "palette.reverse()\n",
    "\n",
    "ax = sns.barplot(data=dem_politician_rank.sort_values(by=\"group_avg_sentiment\", ascending=False), x=\"group_sentiment\", y=\"short_name\", palette=palette, estimator=np.mean, ci=90)\n",
    "\n",
    "ax.set_xlabel(None, fontsize=15)\n",
    "ax.set_ylabel(None)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('rankings/finetuned_gpt2_pred_dem_rank.png', bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3b9c6c-a2e1-43b4-8463-caaff9653fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(5,5)})\n",
    "\n",
    "palette = sns.color_palette(\"Blues\",n_colors=20)\n",
    "palette.reverse()\n",
    "\n",
    "ax = sns.barplot(data=dem_politician_rank.sort_values(by=\"Democrat\", ascending=False), x=\"Democrat\", y=\"short_name\", palette=palette)\n",
    "\n",
    "ax.set_xlabel(None, fontsize=15)\n",
    "ax.set_ylabel(None)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('rankings/gold_dem_rank.png', bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc980b7-aa69-4cbe-8778-f57b463fc909",
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = sns.color_palette(\"Reds\", n_colors=20)\n",
    "palette.reverse()\n",
    "\n",
    "ax = sns.barplot(data=repub_politician_rank.sort_values(by=\"group_avg_sentiment\", ascending=False), x=\"group_sentiment\", y=\"short_name\", palette=palette)\n",
    "\n",
    "ax.set_xlabel(None, fontsize=15)\n",
    "ax.set_ylabel(None)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('rankings/finetuned_gpt2_pred_repub_rank.png', bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1dd3c2-dda4-482e-99b6-6ef4316155ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = sns.color_palette(\"Reds\", n_colors=20)\n",
    "palette.reverse()\n",
    "\n",
    "ax = sns.barplot(data=repub_politician_rank.sort_values(by=\"Republican\", ascending=False), x=\"Republican\", y=\"short_name\", palette=palette)\n",
    "\n",
    "ax.set_xlabel(None, fontsize=15)\n",
    "ax.set_ylabel(None)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('rankings/gold_repub_rank.png', bbox_inches = \"tight\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
