{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "from llments.lm.base.api import APIBasedLM\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_API_RETRY = 10000\n",
    "REQ_TIME_GAP = 4\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"<openai-api-key>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Bias of the LLM Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_prompt(ques, ans1, ans2):\n",
    "    sys_prompt = 'You are a helpful and precise assistant for checking the quality of the answer.'\n",
    "    prompt_template = \"[Prompt]\\n{question}\\n\\n[The Start of Assistant 1's Answer]\\n{answer_1}\\n[The End of Assistant 1's Answer]\\n\\n[The Start of Assistant 2's Answer]\\n{answer_2}\\n[The End of Assistant 2's Answer]\\n\\n[System]\\n{prompt}\\n\"\n",
    "    default_prompt =  \"\"\"We would like to request your feedback on the performance of two AI assistants in response to the prompt displayed above.\n",
    "    Please rate the helpfulness, relevance, accuracy, level of details of their responses. \n",
    "\n",
    "    Each assistant receives an overall score on a scale of 1 to 10, where a higher score indicates better overall performance.\n",
    "    Please first provide a comprehensive explanation of your evaluation, avoiding any potential bias and ensuring that the order in which the responses were presented does not affect your judgment. \n",
    "    Then, output two lines indicating the scores for Assistant 1 and 2, respectively.\n",
    "\n",
    "    Output with the following format:\n",
    "    Evaluation evidence: <your evluation explanation here>\n",
    "    Score of the Assistant 1: <score>\n",
    "    Score of the Assistant 2: <score>\"\"\"\n",
    "    return sys_prompt, prompt_template.format(question=ques, answer_1=ans1, answer_2=ans2, prompt=default_prompt)\n",
    "\n",
    "def query_gpt(system_prompt, user_prompt, eval_model, num_sequences):\n",
    "    try:\n",
    "        base_url = \"https://cmu.litellm.ai\"\n",
    "        responses = APIBasedLM(\"openai/\" + eval_model, base_url).chat_generate(\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt}, \n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature=1,\n",
    "            max_new_tokens=512,\n",
    "            num_return_sequences=num_sequences\n",
    "        )\n",
    "        return responses\n",
    "    except Exception as e:\n",
    "        print(f'Error: {e}')\n",
    "        raise RuntimeError(f\"Failed during query processing.\")\n",
    "    \n",
    "def get_eval(ques, ans1, ans2, eval_model, k, bpc=1):\n",
    "    system_prompt, user_prompt = gen_prompt(ques, ans1, ans2)\n",
    "    responses = query_gpt(system_prompt, user_prompt, eval_model, k)\n",
    "    all_scores = []\n",
    "    contents = []\n",
    "    contents_bpc = []\n",
    "    for response in responses:\n",
    "        for message in response:\n",
    "            if message[\"role\"] == \"assistant\":\n",
    "                content = message['content']\n",
    "                score1, score2 = parse_score_from_review(content)\n",
    "                if score1 == -1 or score2 == -1:\n",
    "                    continue\n",
    "                all_scores.append([score1, score2])\n",
    "                contents.append(content)\n",
    "    \n",
    "    if bpc == 1:\n",
    "        system_prompt, user_prompt_bpc = gen_prompt(ques, ans2, ans1)\n",
    "        responses_bpc = query_gpt(system_prompt, user_prompt_bpc, eval_model, k)\n",
    "        for response in responses_bpc:\n",
    "            for message in response:\n",
    "                if message[\"role\"] == \"assistant\":\n",
    "                    content = message['content']\n",
    "                    score1, score2 = parse_score_from_review(content)\n",
    "                    if score1 == -1 or score2 == -1:\n",
    "                        continue\n",
    "                    all_scores.append([score1, score2])\n",
    "                    contents.append(content)\n",
    "    \n",
    "    if all_scores:\n",
    "        score1 = sum([score[0] for score in all_scores]) / len(all_scores)\n",
    "        score2 = sum([score[1] for score in all_scores]) / len(all_scores)\n",
    "    else:\n",
    "        score1, score2 = -1, -1\n",
    "    return contents, contents_bpc, [score1, score2]\n",
    "\n",
    "def parse_score_from_review(review):\n",
    "    try:\n",
    "        score1 = review.split(\"\\n\")[-2]\n",
    "        score2 = review.split(\"\\n\")[-1]\n",
    "        score1 = score1.split(\":\")[-1].strip()\n",
    "        score2 = score2.split(\":\")[-1].strip()\n",
    "        return [float(score1), float(score2)]\n",
    "    except:\n",
    "        return [-1, -1]\n",
    "    \n",
    "def get_json_list(file_path):\n",
    "    file_path = os.path.expanduser(file_path)\n",
    "    with open(file_path, \"r\") as f:\n",
    "        json_list = []\n",
    "        for line in f:\n",
    "            json_list.append(json.loads(line))\n",
    "        return json_list\n",
    "    \n",
    "def get_text_list(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        return [line.strip() for line in f]\n",
    "    \n",
    "def get_results(m1, m2, eval_model, bpc=0, k=1):\n",
    "    prompts_list = get_text_list(\"responses_communityLM/prompts.txt\")\n",
    "    answer1_jsons = get_json_list(f\"responses_communityLM/{m1}_responses.jsonl\")\n",
    "    answer2_jsons = get_json_list(f\"responses_communityLM/{m2}_responses.jsonl\")\n",
    "    output = f\"review/review_{m1}_vs_{m2}_eval={eval_model.split('/')[-1]}_mec={k}_bpc={bpc}.json\"\n",
    "\n",
    "    assert len(prompts_list) == len(answer1_jsons) == len(answer2_jsons)\n",
    "\n",
    "    reviews = []\n",
    "    total_len = len(prompts_list)\n",
    "    question_idx_list = list(range(total_len))\n",
    "\n",
    "    for i in tqdm(question_idx_list):\n",
    "        assert (\n",
    "            answer1_jsons[i][\"prompt\"]\n",
    "            == answer2_jsons[i][\"prompt\"]\n",
    "        )\n",
    "\n",
    "        ques = prompts_list[i]\n",
    "        ans1 = answer1_jsons[i][\"response\"]\n",
    "        ans2 = answer2_jsons[i][\"response\"]\n",
    "        \n",
    "        reviews.append(get_eval(ques, ans1, ans2, eval_model, k, bpc))\n",
    "        \n",
    "        # To avoid the rate limit set by OpenAI\n",
    "        time.sleep(REQ_TIME_GAP)\n",
    "\n",
    "    model1_vs_model2 = {\n",
    "        'win': 0,\n",
    "        'tie': 0,\n",
    "        'loss': 0\n",
    "    }\n",
    "    with open(f\"{output}\", \"w\") as output_review_file:\n",
    "        for idx, (contents, contents_bpc, [score1, score2]) in enumerate(reviews):\n",
    "            results = {\n",
    "                \"prompt\": prompts_list[idx],\n",
    "                \"review\": contents,\n",
    "                \"review_bpc\": contents_bpc,\n",
    "                \"score\": [score1, score2],\n",
    "            }\n",
    "            output_review_file.write(json.dumps(results) + \"\\n\")\n",
    "            \n",
    "            if score1 == score2:\n",
    "                model1_vs_model2['tie'] += 1\n",
    "                \n",
    "            elif score1 > score2:\n",
    "                model1_vs_model2['win'] += 1\n",
    "            else:\n",
    "                model1_vs_model2['loss'] += 1\n",
    "\n",
    "    print(f'Evaluation results (model1_vs_model2):\\n{model1_vs_model2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Democratic LM vs Republican LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [06:44<00:00,  6.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results (model1_vs_model2):\n",
      "{'win': 20, 'tie': 17, 'loss': 23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "m1=\"democratic\"\n",
    "m2=\"republican\"\n",
    "eval_model=\"neulab/gpt-4o-mini-2024-07-18\"\n",
    "\n",
    "get_results(m1, m2, eval_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Republican LM vs Democratic LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [13:33<00:00, 13.55s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results (model1_vs_model2):\n",
      "{'win': 17, 'tie': 15, 'loss': 28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "m1=\"republican\"\n",
    "m2=\"democratic\"\n",
    "eval_model=\"neulab/gpt-4o-mini-2024-07-18\"\n",
    "\n",
    "get_results(m1, m2, eval_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Democratic LM (fine-tuned LM) vs GPT 2 (pre-trained LM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [07:29<00:00,  7.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results (model1_vs_model2):\n",
      "{'win': 46, 'tie': 1, 'loss': 13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "m2=\"democratic\"\n",
    "m1=\"gpt2\"\n",
    "eval_model=\"neulab/gpt-4o-mini-2024-07-18\"\n",
    "\n",
    "get_results(m1, m2, eval_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [06:49<00:00,  6.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results (model1_vs_model2):\n",
      "{'win': 54, 'tie': 1, 'loss': 5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "m2=\"republican\"\n",
    "m1=\"gpt2\"\n",
    "eval_model=\"neulab/gpt-4o-mini-2024-07-18\"\n",
    "\n",
    "get_results(m1, m2, eval_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llments-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
