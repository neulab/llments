{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "from llments.lm.base.api import APIBasedLM\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_API_RETRY = 10000\n",
    "REQ_TIME_GAP = 4\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"<openai-api-key>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Bias of the LLM Evaluator\n",
    "An evaluation template with three placeholders T (Q, R1, R2), is used to query the LLM for eval- uation. For each testing question q, given two re- sponses r1 and r2 from Assistant 1 and Assistant 2, respectively, the researchers populate these re- sponses into the corresponding slots of the evalu- ation template to form a prompt: T (Q = q, R1 = r1, R2 = r2). The prompt is then used to query the LLM in order to obtain the comparison result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_prompt(ques, ans1, ans2):\n",
    "    \"\"\"Generates a prompt that compares two AI assistants' answers to a question.\n",
    "\n",
    "    Args:\n",
    "        ques (str): The question being asked.\n",
    "        ans1 (str): The first assistant's answer to the question.\n",
    "        ans2 (str): The second assistant's answer to the question.\n",
    "\n",
    "    Returns:\n",
    "        str: A formatted prompt including the question, both answers, and instructions \n",
    "        for evaluation (how to score both assistants).\n",
    "    \"\"\"\n",
    "    sys_prompt = 'You are a helpful and precise assistant for checking the quality of the answer.'\n",
    "    prompt_template = \"[Question]\\n{question}\\n\\n[The Start of Assistant 1's Answer]\\n{answer_1}\\n[The End of Assistant 1's Answer]\\n\\n[The Start of Assistant 2's Answer]\\n{answer_2}\\n[The End of Assistant 2's Answer]\\n\\n[System]\\n{prompt}\\n\"\n",
    "    default_prompt =  \"\"\"We would like to request your feedback on the performance of two AI assistants in response to the user question displayed above.\n",
    "    Please rate the helpfulness, relevance, accuracy, level of details of their responses. \n",
    "\n",
    "    Each assistant receives an overall score on a scale of 1 to 10, where a higher score indicates better overall performance.\n",
    "    Please first provide a comprehensive explanation of your evaluation, avoiding any potential bias and ensuring that the order in which the responses were presented does not affect your judgment. \n",
    "    Then, output two lines indicating the scores for Assistant 1 and 2, respectively.\n",
    "\n",
    "    Output with the following format:\n",
    "    Evaluation evidence: <your evluation explanation here>\n",
    "    Score of the Assistant 1: <score>\n",
    "    Score of the Assistant 2: <score>\"\"\"\n",
    "    return sys_prompt, prompt_template.format(question=ques, answer_1=ans1, answer_2=ans2, prompt=default_prompt)\n",
    "\n",
    "def query_gpt(system_prompt, user_prompt, eval_model, num_sequences):\n",
    "    \"\"\"Queries language model API with the provided prompts.\n",
    "\n",
    "    Args:\n",
    "        system_prompt (str): The system-level prompt setting the context for the responses.\n",
    "        user_prompts (list): A list of prompts (for the user part of the interaction).\n",
    "        eval_model (str): The name of the model to be queried.\n",
    "        num_sequences (int): The number of response sequences to generate for each input.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of responses generated by the language model.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        base_url = \"https://cmu.litellm.ai\"\n",
    "        responses = APIBasedLM(\"openai/\" + eval_model, base_url).chat_generate(\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt}, \n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature=1,\n",
    "            max_new_tokens=512,\n",
    "            num_return_sequences=num_sequences\n",
    "        )\n",
    "        return responses\n",
    "    except Exception as e:\n",
    "        print(f'Error: {e}')\n",
    "        raise RuntimeError(f\"Failed during query processing.\")\n",
    "    \n",
    "def get_eval(ques, ans1, ans2, eval_model, k, bpc=1):\n",
    "    system_prompt, user_prompt = gen_prompt(ques, ans1, ans2)\n",
    "    responses = query_gpt(system_prompt, user_prompt, eval_model, k)\n",
    "    all_scores = []\n",
    "    contents = []\n",
    "    contents_bpc = []\n",
    "    for response in responses:\n",
    "        for message in response:\n",
    "            if message[\"role\"] == \"assistant\":\n",
    "                content = message['content']\n",
    "                score1, score2 = parse_score_from_review(content)\n",
    "                if score1 == -1 or score2 == -1:\n",
    "                    continue\n",
    "                all_scores.append([score1, score2])\n",
    "                contents.append(content)\n",
    "    \n",
    "    if bpc == 1:\n",
    "        system_prompt, user_prompt_bpc = gen_prompt(ques, ans2, ans1)\n",
    "        responses_bpc = query_gpt(eval_model, k, system_prompt, user_prompt_bpc)\n",
    "        for response in responses_bpc:\n",
    "            for message in response:\n",
    "                if message[\"role\"] == \"assistant\":\n",
    "                    content = message['content']\n",
    "                    score1, score2 = parse_score_from_review(content)\n",
    "                    if score1 == -1 or score2 == -1:\n",
    "                        continue\n",
    "                    all_scores.append([score1, score2])\n",
    "                    contents.append(content)\n",
    "    \n",
    "    if all_scores:\n",
    "        score1 = sum([score[0] for score in all_scores]) / len(all_scores)\n",
    "        score2 = sum([score[1] for score in all_scores]) / len(all_scores)\n",
    "    else:\n",
    "        score1, score2 = -1, -1\n",
    "    return contents, contents_bpc, [score1, score2]\n",
    "\n",
    "def parse_score_from_review(review):\n",
    "    \"\"\"Parses the score for two assistants from the review text.\n",
    "\n",
    "    Args:\n",
    "        review (str): The review text that includes the scores.\n",
    "\n",
    "    Returns:\n",
    "        list: A list containing the scores for Assistant 1 and Assistant 2.\n",
    "        If parsing fails, returns [-1, -1].\n",
    "    \"\"\"\n",
    "    try:\n",
    "        score1 = review.split(\"\\n\")[-2]\n",
    "        score2 = review.split(\"\\n\")[-1]\n",
    "        score1 = score1.split(\":\")[-1].strip()\n",
    "        score2 = score2.split(\":\")[-1].strip()\n",
    "        return [float(score1), float(score2)]\n",
    "    except:\n",
    "        return [-1, -1]\n",
    "    \n",
    "def get_json_list(file_path):\n",
    "    \"\"\"Reads a JSON lines file and returns a list of JSON objects.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the JSONL file.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of JSON objects from the file.\n",
    "    \"\"\"\n",
    "    file_path = os.path.expanduser(file_path)\n",
    "    with open(file_path, \"r\") as f:\n",
    "        json_list = []\n",
    "        for line in f:\n",
    "            json_list.append(json.loads(line))\n",
    "        return json_list\n",
    "    \n",
    "def get_results(m1, m2, eval_model, bpc=0, k=1):\n",
    "    \"\"\"Processes results for multiple questions and answers from two assistants.\n",
    "\n",
    "    Args:\n",
    "        m1 (str): Identifier for the first model or assistant.\n",
    "        m2 (str): Identifier for the second model or assistant.\n",
    "        eval_model (str): The evaluation model to be used.\n",
    "        bpc (bool): If True, perform back-and-forth comparisons.\n",
    "        k (int): Number of response sequences to generate.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    question_jsons = get_json_list(\"question.jsonl\")\n",
    "    answer1_jsons = get_json_list(f\"answer/answer_{m1}.jsonl\")\n",
    "    answer2_jsons = get_json_list(f\"answer/answer_{m2}.jsonl\")\n",
    "    output = f\"review/review_{m1}_vs_{m2}_eval={eval_model.split('/')[-1]}_mec={k}_bpc={bpc}.json\"\n",
    "\n",
    "    assert len(question_jsons) == len(answer1_jsons) == len(answer2_jsons)\n",
    "\n",
    "    reviews = []\n",
    "    total_len = len(question_jsons)\n",
    "    question_idx_list = list(range(total_len))\n",
    "\n",
    "    for i in tqdm(question_idx_list):\n",
    "        assert (\n",
    "            answer1_jsons[i][\"question_id\"]\n",
    "            == question_jsons[i][\"question_id\"]\n",
    "            == answer2_jsons[i][\"question_id\"]\n",
    "        )\n",
    "\n",
    "        ques = question_jsons[i][\"text\"]\n",
    "        ans1 = answer1_jsons[i][\"text\"]\n",
    "        ans2 = answer2_jsons[i][\"text\"]\n",
    "        \n",
    "        reviews.append(get_eval(ques, ans1, ans2, eval_model, k, bpc))\n",
    "        \n",
    "        # To avoid the rate limit set by OpenAI\n",
    "        time.sleep(REQ_TIME_GAP)\n",
    "\n",
    "    model1_vs_model2 = {\n",
    "        'win': 0,\n",
    "        'tie': 0,\n",
    "        'loss': 0\n",
    "    }\n",
    "    with open(f\"{output}\", \"w\") as output_review_file:\n",
    "        for idx, (contents, contents_bpc, [score1, score2]) in enumerate(reviews):\n",
    "            results = {\n",
    "                \"question_id\": question_jsons[idx][\"question_id\"],\n",
    "                \"question\": question_jsons[idx][\"text\"],\n",
    "                \"review\": contents,\n",
    "                \"review_bpc\": contents_bpc,\n",
    "                \"score\": [score1, score2],\n",
    "            }\n",
    "            output_review_file.write(json.dumps(results) + \"\\n\")\n",
    "            \n",
    "            if score1 == score2:\n",
    "                model1_vs_model2['tie'] += 1\n",
    "                \n",
    "            elif score1 > score2:\n",
    "                model1_vs_model2['win'] += 1\n",
    "            else:\n",
    "                model1_vs_model2['loss'] += 1\n",
    "\n",
    "    print(f'Evaluation results (model1_vs_model2):\\n{model1_vs_model2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Win Rate of Vicuna-13B significantly fluctuates when positioned as Assistant 1 and Assistant 2.\n",
    "Conflict Rate refers to the proportion of conflicting results given by the same evaluator when simply changing the position of two models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vicuna-13B v.s. ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [09:52<00:00,  7.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results (model1_vs_model2):\n",
      "{'win': 41, 'tie': 0, 'loss': 39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "m1=\"gpt35\"\n",
    "m2=\"vicuna-13b\"\n",
    "eval_model=\"neulab/gpt-4o-mini-2024-07-18\"\n",
    "\n",
    "get_results(m1, m2, eval_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [09:39<00:00,  7.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results (model1_vs_model2):\n",
      "{'win': 36, 'tie': 1, 'loss': 43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "m1=\"vicuna-13b\"\n",
    "m2=\"gpt35\"\n",
    "eval_model=\"neulab/gpt-4o-mini-2024-07-18\"\n",
    "\n",
    "get_results(m1, m2, eval_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vicuna-13B v.s. ChatGPT | Evaluator: ChatGPT\n",
      "Vicuna-13b win rate as assistant 1: 45.0%\n",
      "Vicuna-13b win rate as assistant 2: 48.75%\n",
      "Conflict rate: 13/80 (16.25%)\n"
     ]
    }
   ],
   "source": [
    "gpt35_vs_vicuna13b_results = []\n",
    "\n",
    "with open('review/review_gpt35_vs_vicuna-13b_eval=gpt-4o-mini-2024-07-18_mec=1_bpc=0.json', 'r') as file:\n",
    "    for line in file:\n",
    "        json_object = json.loads(line)\n",
    "        gpt35_vs_vicuna13b_results.append(json_object)\n",
    "\n",
    "vicuna13b_vs_gpt35_results = []\n",
    "\n",
    "with open('review/review_vicuna-13b_vs_gpt35_eval=gpt-4o-mini-2024-07-18_mec=1_bpc=0.json', 'r') as file:\n",
    "    for line in file:\n",
    "        json_object = json.loads(line)\n",
    "        vicuna13b_vs_gpt35_results.append(json_object)\n",
    "\n",
    "vicuna13b_win_rate_first = 0\n",
    "vicuna13b_win_rate_second = 0\n",
    "conflict = 0\n",
    "\n",
    "for i in range(len(gpt35_vs_vicuna13b_results)):\n",
    "    vicuna_as_first_winner = False\n",
    "    vicuna_as_second_winner = False\n",
    "    if gpt35_vs_vicuna13b_results[i]['score'][0] < gpt35_vs_vicuna13b_results[i]['score'][1]:\n",
    "        vicuna13b_win_rate_second += 1\n",
    "        vicuna_as_second_winner = True\n",
    "    if vicuna13b_vs_gpt35_results[i]['score'][0] > vicuna13b_vs_gpt35_results[i]['score'][1]:\n",
    "        vicuna13b_win_rate_first += 1\n",
    "        vicuna_as_first_winner = True\n",
    "    if vicuna_as_first_winner != vicuna_as_second_winner:\n",
    "        conflict += 1\n",
    "\n",
    "\n",
    "print(\"Vicuna-13B v.s. ChatGPT | Evaluator: ChatGPT\")\n",
    "print(f\"Vicuna-13b win rate as assistant 1: {vicuna13b_win_rate_first / 80 * 100}%\")\n",
    "print(f\"Vicuna-13b win rate as assistant 2: {vicuna13b_win_rate_second / 80 * 100}%\")\n",
    "print(f\"Conflict rate: {conflict}/80 ({conflict / 80 * 100}%)\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vicuna-13B v.s. Alpaca-13B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [16:30<00:00, 12.38s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results (model1_vs_model2):\n",
      "{'win': 4, 'tie': 0, 'loss': 76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "m1=\"alpaca-13b\"\n",
    "m2=\"vicuna-13b\"\n",
    "eval_model=\"neulab/gpt-4o-mini-2024-07-18\"\n",
    "\n",
    "get_results(m1, m2, eval_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [16:17<00:00, 12.21s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results (model1_vs_model2):\n",
      "{'win': 77, 'tie': 0, 'loss': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "m1=\"vicuna-13b\"\n",
    "m2=\"alpaca-13b\"\n",
    "eval_model=\"neulab/gpt-4o-mini-2024-07-18\"\n",
    "\n",
    "get_results(m1, m2, eval_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vicuna-13B v.s. Alpaca-13B | Evaluator: ChatGPT\n",
      "Vicuna-13b win rate as assistant 1: 96.25%\n",
      "Vicuna-13b win rate as assistant 2: 95.0%\n",
      "Conflict rate: 3/80 (3.75%)\n"
     ]
    }
   ],
   "source": [
    "alpaca13b_vs_vicuna13b_results = []\n",
    "\n",
    "with open('review/review_alpaca-13b_vs_vicuna-13b_eval=gpt-4o-mini-2024-07-18_mec=1_bpc=0.json', 'r') as file:\n",
    "    for line in file:\n",
    "        json_object = json.loads(line)\n",
    "        alpaca13b_vs_vicuna13b_results.append(json_object)\n",
    "\n",
    "vicuna13b_vs_alpaca13b_results = []\n",
    "\n",
    "with open('review/review_vicuna-13b_vs_alpaca-13b_eval=gpt-4o-mini-2024-07-18_mec=1_bpc=0.json', 'r') as file:\n",
    "    for line in file:\n",
    "        json_object = json.loads(line)\n",
    "        vicuna13b_vs_alpaca13b_results.append(json_object)\n",
    "\n",
    "vicuna13b_win_rate_first = 0\n",
    "vicuna13b_win_rate_second = 0\n",
    "conflict = 0\n",
    "\n",
    "for i in range(len(gpt35_vs_vicuna13b_results)):\n",
    "    vicuna_as_first_winner = False\n",
    "    vicuna_as_second_winner = False\n",
    "    if alpaca13b_vs_vicuna13b_results[i]['score'][0] < alpaca13b_vs_vicuna13b_results[i]['score'][1]:\n",
    "        vicuna13b_win_rate_second += 1\n",
    "        vicuna_as_second_winner = True\n",
    "    if vicuna13b_vs_alpaca13b_results[i]['score'][0] > vicuna13b_vs_alpaca13b_results[i]['score'][1]:\n",
    "        vicuna13b_win_rate_first += 1\n",
    "        vicuna_as_first_winner = True\n",
    "    if vicuna_as_first_winner != vicuna_as_second_winner:\n",
    "        conflict += 1\n",
    "\n",
    "\n",
    "print(\"Vicuna-13B v.s. Alpaca-13B | Evaluator: ChatGPT\")\n",
    "print(f\"Vicuna-13b win rate as assistant 1: {vicuna13b_win_rate_first / 80 * 100}%\")\n",
    "print(f\"Vicuna-13b win rate as assistant 2: {vicuna13b_win_rate_second / 80 * 100}%\")\n",
    "print(f\"Conflict rate: {conflict}/80 ({conflict / 80 * 100}%)\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llments-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
