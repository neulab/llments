{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BiasMonkey\n",
    "\n",
    "This is a replication of the experiments from [BiasMonkey](https://arxiv.org/abs/2311.04076) (Tjuatja et al. 2023), which investigates whether LLMs exhibit human-like response biases in survey questionnaires, based on the [original repo](https://github.com/lindiatjuatja/BiasMonkey).\n",
    "\n",
    "Before running the notebook, please install requirements and download the prompts by cloning the original repo.\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "git clone https://github.com/lindiatjuatja/BiasMonkey\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import glob\n",
    "from llments.lm.base.hugging_face import HuggingFaceLM\n",
    "from BiasMonkey.utils import Bias, Response\n",
    "import torch, gc\n",
    "from bias_monkey_utils import generate_survey_responses\n",
    "\n",
    "device = 'cuda:0'  # change to 'mps' if you have a mac, or 'cuda:0' if you have an NVIDIA GPU \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "836c8f8b0ecf4a2eae3352cdea18cfe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# bias_types = ['acquiescence','response_order', 'odd_even', 'allow_forbid', 'opinion_float'] \n",
    "# perturbations = ['-key_typo', '-middle_random', '-letter_swap']\n",
    "\n",
    "base_models = ['llama2-7b', 'llama2-13b', 'llama2-70b']\n",
    "chat_models = ['llama2-7b-chat', 'llama2-13b-chat', 'llama2-70b-chat']\n",
    "gpt_models = ['gpt-3.5-turbo', 'gpt-3.5-turbo-instruct']\n",
    "\n",
    "# os.environ['HF_HOME'] = '/data/datasets/hf_cache/hub/'\n",
    "# models = ['meta-llama/Llama-2-7b-chat-hf']\n",
    "models = ['/data/models/huggingface/meta-llama/Llama-2-7b-chat-hf/']\n",
    "for csv_file in glob.glob('BiasMonkey/prompts/response_order.csv'):\n",
    "    bias_type = os.path.basename(csv_file.removesuffix(\".csv\")).split('-')[0]\n",
    "    for model in models:\n",
    "        lm = HuggingFaceLM(model, device=device)\n",
    "        output_path = f'results/{model}/{os.path.basename(csv_file).split(\".\")[0]}.pickle'\n",
    "        generate_survey_responses(lm, csv_file, bias_type, output_path, is_chat_model=True, seed=1, num_samples=50, overwrite=True)\n",
    "        # free up memory from GPU\n",
    "        del lm\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate_survey_responses: model=<llments.lm.base.hugging_face.HuggingFaceLM object at 0x7f9f1d18a000>, prompts_file=BiasMonkey/prompts/response_order.csv, bias_type=response_order, output_path=results//data/models/huggingface/meta-llama/Llama-2-7b-chat-hf//response_order.pickle, seed=1, num_samples=50, overwrite=True, prompt_template=Please answer the following question with one of the alphabetical options provided.\n",
      "Question: \n",
      "Generating responses for prompt3:\n",
      "[{'role': 'system', 'content': 'Respond only with the letter corresponding to your answer.'}, {'role': 'user', 'content': 'Please answer the following question with one of the alphabetical options provided.\\nQuestion: Over the next 30 years, do you think that the average American family will see its standard of living\\nA. Get better\\nB. Stay about the same\\nC. Get worse'}]\n",
      "Generating responses for prompt3:\n",
      "[{'role': 'system', 'content': 'Respond only with the letter corresponding to your answer.'}, {'role': 'user', 'content': 'Please answer the following question with one of the alphabetical options provided.\\nQuestion: Over the next 30 years, do you think that the average American family will see its standard of living\\nA. Get worse\\nB. Stay about the same\\nC. Get better'}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "output_path = f'results/{model}/{os.path.basename(csv_file).split(\".\")[0]}.pickle'\n",
    "generate_survey_responses(lm, csv_file, bias_type, output_path, is_chat_model=True, seed=1, num_samples=50, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['b,b,a,a,a,b,b,b,b,a,b,b,a,b,b,b,b,a,b,b,b,b,b,b,b,b,b,b,b,b,b,b,a,b,a,b,b,b,b,b,b,b,b,b,b,b,b,b,b,b'\n",
      " 'b,c,c,c,b,c,c,c,c,c,c,c,b,b,c,b,c,b,c,c,c,c,b,c,b,c,c,c,c,b,c,c,c,c,c,c,c,c,b,c,b,c,c,c,c,b,c,c,c,c']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_pickle(output_path)\n",
    "# print all values in responses column\n",
    "print(df['responses'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from BiasMonkey.format_results import format_df\n",
    "\n",
    "# convert the pickle files to csv\n",
    "\n",
    "models = ['/data/models/huggingface/meta-llama/Llama-2-7b-chat-hf/']\n",
    "for model in models:\n",
    "    dir = f\"results/{model}\"\n",
    "    Path(f\"{dir}/csv\").mkdir(parents=True, exist_ok=True)\n",
    "    for filename in os.listdir(f\"results/{model}\"):\n",
    "        print(filename)\n",
    "        name = filename.split('.')[0]\n",
    "        f = os.path.join(dir, filename)\n",
    "        if os.path.isfile(f):\n",
    "            df = format_df(filename)\n",
    "            df.to_csv(f'results/{model}/csv/{name}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llments",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
