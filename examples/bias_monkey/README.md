# BiasMonkey

This is a replication of the experiments from
[BiasMonkey](https://arxiv.org/abs/2311.04076) (Tjuatja et al. 2023),
which investigates whether LLMs exhibit human-like response biases
in survey questionnaires.

## Reference

Much of the code is derived from the
[BiasMonkey repo](https://github.com/lindiatjuatja/BiasMonkey).

If you use this example, we would appreciate if you acknowledge
[LLMents](https://github.com/neulab/llments) and the original paper.

```bibtex
@misc{
    title = "{LLMents}: A Toolkit for Language Model Experiments",
    author = "
      Graham Neubig and
      Aakriti Kinra and
      Mihir Bansal and
      Qingyang Liu and
      Rohan Modi and
      Xinran Wan
    ",
    year = "2024",
    howpublished = "https://github.com/neulab/llments",
}
```

```bibtex
@misc{tjuatja2024llms,
      title={Do LLMs exhibit human-like response biases?
        A case study in survey design},
      author={Lindia Tjuatja and Valerie Chen and Sherry Tongshuang Wu and
        Ameet Talwalkar and Graham Neubig},
      year={2024},
      eprint={2311.04076},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```
