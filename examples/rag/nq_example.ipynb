{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8ca8370c",
      "metadata": {
        "id": "8ca8370c"
      },
      "source": [
        "# Retrieval Augmented Generation on the Natural Questions dataset\n",
        "\n",
        "This is an example of RAG for Natural Questions dataset.\n",
        "```bash\n",
        "pip install -r requirements.txt\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "29563e5d-41b0-4f89-8d8b-a54b40f8dfb7",
      "metadata": {
        "id": "29563e5d-41b0-4f89-8d8b-a54b40f8dfb7"
      },
      "outputs": [],
      "source": [
        "from llments.datastore.pyserini_datastore import PyseriniDatastore\n",
        "from llments.eval.rag import QAEvaluator\n",
        "from llments.eval.rag import QAEvalContext\n",
        "from llments.lm.base.hugging_face import HuggingFaceLM\n",
        "from llments.lm.rag import RAGLanguageModel\n",
        "from tqdm.auto import tqdm\n",
        "import json\n",
        "import statistics\n",
        "import torch\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "43943fd0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA is available! Using GPU.\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")          # GPU available, select GPU as device\n",
        "    print(\"CUDA is available! Using GPU.\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")           # No GPU available, fall back to CPU\n",
        "    print(\"CUDA is not available. Using CPU.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0022efe",
      "metadata": {
        "id": "d0022efe"
      },
      "source": [
        "## Encode the Documents file provided in jsonl format\n",
        "\n",
        "The following code generates an encoding for Documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "931a71d1",
      "metadata": {
        "id": "931a71d1"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cbe3f86e40c4447fb25adf145c3f8a1b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "language_model = HuggingFaceLM('mistralai/Mistral-7B-v0.1', device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "788b7b0d",
      "metadata": {},
      "source": [
        "### Trial for RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "d84652e5",
      "metadata": {},
      "outputs": [],
      "source": [
        "datastore = PyseriniDatastore(index_path='/data/tir/projects/tir7/user_data/mihirban/NQ/colbert/NQ_index_passage-0', document_path='/data/tir/projects/tir7/user_data/mihirban/NQ/wiki_par.jsonl', index_encoder='colbert-ir/colbertv2.0', fields=['contents'], docid_field=\"id\", pooling='mean', to_faiss=True, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "5b8e218f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5b8e218f",
        "outputId": "17bfc02b-9cc0-4495-e9d5-ee9ad37771c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading the index...\n",
            "Index loaded successfully!\n",
            "Loading the document file...\n",
            "Documents loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "rag_LM = RAGLanguageModel(base=language_model, datastore=datastore, max_results=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "f6bce21e",
      "metadata": {},
      "outputs": [],
      "source": [
        "rag_LM.set_max_results(max_results=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "456ecd6c",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        }
      ],
      "source": [
        "lm_response = rag_LM.generate(condition='when did the east india company take control of india?', max_new_tokens=10, temperature=0.7, num_return_sequences=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "6c5f3c75",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response 0 : 1757\n"
          ]
        }
      ],
      "source": [
        "for i in range(0, len(lm_response)):\n",
        "    print(f\"Response {i} : \" + lm_response[i].split(\"\\n\")[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f54ff76d",
      "metadata": {},
      "source": [
        "### Trial for baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "c7e0a705",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        }
      ],
      "source": [
        "condition = \"when did the east india company take control of india?\"\n",
        "prompt = \"Please answer the following question.\\nQuestion: \" + condition + \"\\nAnswer: \"\n",
        "lm_response = [x.split(\"Answer: \")[1].strip() for x in language_model.generate(condition=prompt, max_new_tokens=50, temperature=0.7, num_return_sequences=1)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "e7ede0f5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response 0 : 1757\n"
          ]
        }
      ],
      "source": [
        "for i in range(0, len(lm_response)):\n",
        "    print(f\"Response {i} : \" + lm_response[i].split(\"\\n\")[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b48958b5",
      "metadata": {},
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "7637d38b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the JSON file\n",
        "with open('/data/tir/projects/tir7/user_data/mihirban/NQ/gold_nq_zeno_file.json') as f:\n",
        "    data = json.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7b46205",
      "metadata": {},
      "source": [
        "### Baseline (without RAG)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "f3deb44b",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2db3587d83a3465ba08f8e057ec9ad5f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processing:   0%|          | 0/2837 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        }
      ],
      "source": [
        "answer_set_list = []\n",
        "outputs = []\n",
        "x = 0\n",
        "\n",
        "# Iterate over each item in the JSON data with tqdm\n",
        "with tqdm(data, desc=\"Processing\") as pbar:\n",
        "    for item in pbar:\n",
        "        # Extract answers and input for each item\n",
        "        answer_set = item['output']['answer_set']\n",
        "        condition = item['input'] + \"?\"\n",
        "        prompt = \"Please answer the following question.\\nQuestion: \" + condition + \"\\nAnswer: \"\n",
        "        lm_response = [x.split(\"Answer: \")[1].strip() for x in language_model.generate(condition=prompt, max_new_tokens=100, temperature=0.7, num_return_sequences=1)]\n",
        "\n",
        "        outputs.append(lm_response[0].split(\"\\nQuestion\")[0])\n",
        "        answer_set_list.append(QAEvalContext(answer_set))\n",
        "        if x == 5:\n",
        "            break\n",
        "        x += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "67c95fe9",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 6/6 [00:00<00:00, 5837.58it/s]\n",
            "Evaluating: 100%|██████████| 6/6 [00:00<00:00, 29127.11it/s]\n",
            "Evaluating: 100%|██████████| 6/6 [00:00<00:00, 8895.66it/s]\n",
            "Evaluating: 100%|██████████| 6/6 [00:00<00:00, 1788.62it/s]\n"
          ]
        }
      ],
      "source": [
        "evaluator_f1 = QAEvaluator(metric='f1')\n",
        "f1 = evaluator_f1.evaluate_batch(hyps=outputs, contexts=answer_set_list, show_progress=True)\n",
        "\n",
        "evaluator_accuracy = QAEvaluator(metric='accuracy')\n",
        "acc = evaluator_accuracy.evaluate_batch(hyps=outputs, contexts=answer_set_list, show_progress=True)\n",
        "\n",
        "evaluator_exact_match = QAEvaluator(metric='exact_match')\n",
        "em_acc = evaluator_exact_match.evaluate_batch(hyps=outputs, contexts=answer_set_list, show_progress=True)\n",
        "\n",
        "evaluator_rougel = QAEvaluator(metric='rougel')\n",
        "rouge = evaluator_rougel.evaluate_batch(hyps=outputs, contexts=answer_set_list, show_progress=True)\n",
        "\n",
        "mean_f1 = statistics.mean(f1)\n",
        "mean_acc = statistics.mean(acc)\n",
        "mean_em_acc = statistics.mean(em_acc)\n",
        "mean_rouge = statistics.mean(rouge)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "5e1f2f8c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean F1: 0.27586659886431053\n",
            "Mean Accuracy: 0.16666666666666666\n",
            "Mean Exact Match Accuracy: 0.16666666666666666\n",
            "Mean ROUGE-L: 0.3047501501134335\n"
          ]
        }
      ],
      "source": [
        "print(\"Mean F1:\", mean_f1)\n",
        "print(\"Mean Accuracy:\", mean_acc)\n",
        "print(\"Mean Exact Match Accuracy:\", mean_em_acc)\n",
        "print(\"Mean ROUGE-L:\", mean_rouge)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e6765a7",
      "metadata": {},
      "source": [
        "### RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "10a69bdd",
      "metadata": {},
      "outputs": [],
      "source": [
        "datastore_nq = PyseriniDatastore(index_path='/data/tir/projects/tir7/user_data/mihirban/NQ/colbert/NQ_index_passage-0', document_path='/data/tir/projects/tir7/user_data/mihirban/NQ/wiki_par.jsonl', index_encoder='colbert-ir/colbertv2.0', fields=['contents'], docid_field=\"id\", pooling='mean', to_faiss=True, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "e0e44e6f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading the index...\n",
            "Index loaded successfully!\n",
            "Loading the document file...\n",
            "Documents loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "rag_LM_nq = RAGLanguageModel(base=language_model, datastore=datastore_nq, max_results=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "35e539af",
      "metadata": {},
      "outputs": [],
      "source": [
        "rag_LM_nq.set_max_results(max_results=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "03e7e702",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5c86dec587e54a9bb0de061773d2a7a8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processing:   0%|          | 0/2837 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3 dots mean that the number is infinite.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1. Paul McCartney\n",
            "2. John Lennon\n",
            "3. George Harrison\n",
            "4. Ringo Starr\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1. Coldplay\n",
            "2. Beyonce\n",
            "3. Bruno Mars\n",
            "4. Rihanna\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1. Davos, Switzerland.\n",
            "2. Osaka, Japan.\n",
            "3. New York, USA.\n",
            "4. London, UK.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1. Redwood National Park is located in the northwestern corner of California, along the coast.\n",
            "2. The park is made up of four separate sections: Del Norte Coast, Jedediah Smith, Prairie Creek, and Redwood Creek.\n",
            "3. The park is home to the tallest trees in the world, the giant redwoods.\n",
            "4. The park is also home to a variety of other plants and animals, including black bears, elk, and\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "669\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "answer_set_list = []\n",
        "outputs = []\n",
        "x = 0\n",
        "import re\n",
        "\n",
        "# Iterate over each item in the JSON data with tqdm\n",
        "with tqdm(data, desc=\"Processing\") as pbar:\n",
        "    for item in pbar:\n",
        "        # Extract answers and input for each item\n",
        "        answer_set = item['output']['answer_set']\n",
        "        condition = item['input'] + \"?\"\n",
        "\n",
        "        lm_response = [x.split(\"Answer: \")[1].strip() for x in rag_LM_nq.generate(condition=condition, max_new_tokens=100, temperature=0.7, num_return_sequences=1)]\n",
        "\n",
        "        outputs.append(re.split(\"Please answer the following question.|\\nQuestion|\\nContext\", lm_response[0])[0])\n",
        "        print(re.split(\"Please answer the following question.|\\nQuestion|\\nContext\", lm_response[0])[0])\n",
        "        answer_set_list.append(QAEvalContext(answer_set))\n",
        "        if x == 5:\n",
        "            break\n",
        "        x += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "eeee9ba0",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 6/6 [00:00<00:00, 7509.94it/s]\n",
            "Evaluating: 100%|██████████| 6/6 [00:00<00:00, 32181.36it/s]\n",
            "Evaluating: 100%|██████████| 6/6 [00:00<00:00, 9221.63it/s]\n",
            "Evaluating: 100%|██████████| 6/6 [00:00<00:00, 1904.91it/s]\n"
          ]
        }
      ],
      "source": [
        "evaluator_f1 = QAEvaluator(metric='f1')\n",
        "f1 = evaluator_f1.evaluate_batch(hyps=outputs, contexts=answer_set_list, show_progress=True)\n",
        "\n",
        "evaluator_accuracy = QAEvaluator(metric='accuracy')\n",
        "acc = evaluator_accuracy.evaluate_batch(hyps=outputs, contexts=answer_set_list, show_progress=True)\n",
        "\n",
        "evaluator_exact_match = QAEvaluator(metric='exact_match')\n",
        "em_acc = evaluator_exact_match.evaluate_batch(hyps=outputs, contexts=answer_set_list, show_progress=True)\n",
        "\n",
        "evaluator_rougel = QAEvaluator(metric='rougel')\n",
        "rouge = evaluator_rougel.evaluate_batch(hyps=outputs, contexts=answer_set_list, show_progress=True)\n",
        "\n",
        "mean_f1 = statistics.mean(f1)\n",
        "mean_acc = statistics.mean(acc)\n",
        "mean_em_acc = statistics.mean(em_acc)\n",
        "mean_rouge = statistics.mean(rouge)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "dcfc5662",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean F1: 0.1711995733734864\n",
            "Mean Accuracy: 0\n",
            "Mean Exact Match Accuracy: 0.0\n",
            "Mean ROUGE-L: 0.16416916158942982\n"
          ]
        }
      ],
      "source": [
        "print(\"Mean F1:\", mean_f1)\n",
        "print(\"Mean Accuracy:\", mean_acc)\n",
        "print(\"Mean Exact Match Accuracy:\", mean_em_acc)\n",
        "print(\"Mean ROUGE-L:\", mean_rouge)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "d829f6bf",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['3 dots mean that the number is infinite.\\n',\n",
              " '1. Paul McCartney\\n2. John Lennon\\n3. George Harrison\\n4. Ringo Starr\\n',\n",
              " '1. Coldplay\\n2. Beyonce\\n3. Bruno Mars\\n4. Rihanna\\n',\n",
              " '1. Davos, Switzerland.\\n2. Osaka, Japan.\\n3. New York, USA.\\n4. London, UK.\\n',\n",
              " '1. Redwood National Park is located in the northwestern corner of California, along the coast.\\n2. The park is made up of four separate sections: Del Norte Coast, Jedediah Smith, Prairie Creek, and Redwood Creek.\\n3. The park is home to the tallest trees in the world, the giant redwoods.\\n4. The park is also home to a variety of other plants and animals, including black bears, elk, and',\n",
              " '669\\n\\n']"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "3b570906",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[QAEvalContext(gold_answers=['the therefore sign ( ∴ ) is generally used before a logical consequence , such as the conclusion of a syllogism', 'a logical consequence , such as the conclusion of a syllogism', 'therefore sign', 'the therefore sign']),\n",
              " QAEvalContext(gold_answers=['George Harrison', 'Richard Starkey', 'Ringo Starr']),\n",
              " QAEvalContext(gold_answers=['Beyoncé', 'Coldplay with special guest performers Beyoncé and Bruno Mars', 'British rock group Coldplay with special guest performers Beyoncé and Bruno Mars', 'Bruno Mars', 'Coldplay']),\n",
              " QAEvalContext(gold_answers=['Davos', 'Davos , a mountain resort in Graubünden , in the eastern Alps region of Switzerland']),\n",
              " QAEvalContext(gold_answers=['from the northern California coast north to the southern Oregon Coast', 'the coast of northern California', 'Del Norte County', 'Humboldt County']),\n",
              " QAEvalContext(gold_answers=['Gareth Barry'])]"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "answer_set_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a1d2c20",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
