{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8ca8370c",
      "metadata": {
        "id": "8ca8370c"
      },
      "source": [
        "# Retrieval Augmented Generation on the Natural Questions dataset\n",
        "\n",
        "This is an example of RAG for Natural Questions dataset.\n",
        "```bash\n",
        "pip install -r requirements.txt\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "29563e5d-41b0-4f89-8d8b-a54b40f8dfb7",
      "metadata": {
        "id": "29563e5d-41b0-4f89-8d8b-a54b40f8dfb7"
      },
      "outputs": [],
      "source": [
        "from llments.datastore.pyserini_datastore import PyseriniDatastore\n",
        "from llments.eval.rag import RAGEvaluator\n",
        "from llments.eval.rag import RAGEvalContext\n",
        "from llments.lm.base.hugging_face import HuggingFaceLM\n",
        "from llments.lm.rag import RAGLanguageModel\n",
        "from tqdm.auto import tqdm\n",
        "import json\n",
        "import statistics\n",
        "import torch\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "43943fd0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA is available! Using GPU.\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")          # GPU available, select GPU as device\n",
        "    print(\"CUDA is available! Using GPU.\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")           # No GPU available, fall back to CPU\n",
        "    print(\"CUDA is not available. Using CPU.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0022efe",
      "metadata": {
        "id": "d0022efe"
      },
      "source": [
        "## Encode the Documents file provided in jsonl format\n",
        "\n",
        "The following code generates an encoding for Documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "931a71d1",
      "metadata": {
        "id": "931a71d1"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0a95a8db53f9415f83de1091e7a41e28",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "language_model = HuggingFaceLM('mistralai/Mistral-7B-v0.1', device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "788b7b0d",
      "metadata": {},
      "source": [
        "### Trial for RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d84652e5",
      "metadata": {},
      "outputs": [],
      "source": [
        "datastore = PyseriniDatastore(index_path='examples/rag/NQ_index_passages', document_path='/data/tir/projects/tir7/user_data/mihirban/NQ/wiki_par.jsonl', index_encoder='colbert-ir/colbertv2.0', fields=['contents'], docid_field=\"id\", pooling='mean', to_faiss=True, device=device, shard_id=0 ,shard_num=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "5b8e218f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5b8e218f",
        "outputId": "17bfc02b-9cc0-4495-e9d5-ee9ad37771c3"
      },
      "outputs": [],
      "source": [
        "rag_LM = RAGLanguageModel(base=language_model, datastore=datastore, max_results=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "456ecd6c",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        }
      ],
      "source": [
        "lm_response = rag_LM.generate(condition='when did the east india company take control of india?', max_new_tokens=30, temperature=0.7, num_return_sequences=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "6c5f3c75",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response 0 : 28 March – Bombay is granted to the East India Company.\n",
            "\n",
            "Question: when did the East India Company take control of india?\n"
          ]
        }
      ],
      "source": [
        "for i in range(0, len(lm_response)):\n",
        "    print(f\"Response {i} : \" + lm_response[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f54ff76d",
      "metadata": {},
      "source": [
        "### Trial for baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "c7e0a705",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        }
      ],
      "source": [
        "condition = \"when did the east india company take control of india?\"\n",
        "prompt = \"Please answer the following question.\\nQuestion: \" + condition + \"\\nAnswer: \"\n",
        "lm_response = [x.split(\"Answer: \")[1].strip() for x in language_model.generate(condition=prompt, max_new_tokens=10, temperature=0.7, num_return_sequences=1)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "e7ede0f5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response 0 : 1757\n",
            "\n",
            "Question: who was\n"
          ]
        }
      ],
      "source": [
        "for i in range(0, len(lm_response)):\n",
        "    print(f\"Response {i} : \" + lm_response[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b48958b5",
      "metadata": {},
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "7637d38b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the JSON file\n",
        "with open('/data/tir/projects/tir7/user_data/mihirban/NQ/gold_nq_zeno_file.json') as f:\n",
        "    data = json.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7b46205",
      "metadata": {},
      "source": [
        "### Baseline (without RAG)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "f3deb44b",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "01534de8ec5c465f91dc7abe2352ba31",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processing:   0%|          | 0/2837 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        }
      ],
      "source": [
        "answer_set_list = []\n",
        "outputs = []\n",
        "x = 0\n",
        "\n",
        "# Iterate over each item in the JSON data with tqdm\n",
        "with tqdm(data, desc=\"Processing\") as pbar:\n",
        "    for item in pbar:\n",
        "        # Extract answers and input for each item\n",
        "        answer_set = item['output']['answer_set']\n",
        "        condition = item['input'] + \"?\"\n",
        "        prompt = \"Please answer the following question.\\nQuestion: \" + condition + \"\\nAnswer: \"\n",
        "        lm_response = [x.split(\"Answer: \")[1].strip() for x in language_model.generate(condition=prompt, max_new_tokens=10, temperature=0.7, num_return_sequences=1)]\n",
        "\n",
        "        outputs.append(lm_response[0])\n",
        "        answer_set_list.append(RAGEvalContext(answer_set))\n",
        "        if x == 50:\n",
        "            break\n",
        "        x += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "67c95fe9",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 51/51 [00:00<00:00, 7906.47it/s]\n",
            "Evaluating: 100%|██████████| 51/51 [00:00<00:00, 86253.83it/s]\n",
            "Evaluating: 100%|██████████| 51/51 [00:00<00:00, 27831.06it/s]\n",
            "Evaluating: 100%|██████████| 51/51 [00:00<00:00, 4799.19it/s]\n"
          ]
        }
      ],
      "source": [
        "evaluator_f1 = RAGEvaluator(metric='f1')\n",
        "f1 = evaluator_f1.evaluate_batch(hyps=outputs, contexts=answer_set_list, show_progress=True)\n",
        "\n",
        "evaluator_accuracy = RAGEvaluator(metric='accuracy')\n",
        "acc = evaluator_accuracy.evaluate_batch(hyps=outputs, contexts=answer_set_list, show_progress=True)\n",
        "\n",
        "evaluator_exact_match = RAGEvaluator(metric='exact_match')\n",
        "em_acc = evaluator_exact_match.evaluate_batch(hyps=outputs, contexts=answer_set_list, show_progress=True)\n",
        "\n",
        "evaluator_rougel = RAGEvaluator(metric='rougel')\n",
        "rouge = evaluator_rougel.evaluate_batch(hyps=outputs, contexts=answer_set_list, show_progress=True)\n",
        "\n",
        "mean_f1 = statistics.mean(f1)\n",
        "mean_acc = statistics.mean(acc)\n",
        "mean_em_acc = statistics.mean(em_acc)\n",
        "mean_rouge = statistics.mean(rouge) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "5e1f2f8c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean F1: 0.17200647935942054\n",
            "Mean Accuracy: 0\n",
            "Mean Exact Match Accuracy: 0\n",
            "Mean ROUGE-L: 0.1363216288018862\n"
          ]
        }
      ],
      "source": [
        "print(\"Mean F1:\", mean_f1)\n",
        "print(\"Mean Accuracy:\", mean_acc)\n",
        "print(\"Mean Exact Match Accuracy:\", mean_em_acc)\n",
        "print(\"Mean ROUGE-L:\", mean_rouge)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e6765a7",
      "metadata": {},
      "source": [
        "### RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03e7e702",
      "metadata": {},
      "outputs": [],
      "source": [
        "answer_set_list = []\n",
        "outputs = []\n",
        "rag_LM = RAGLanguageModel(base=language_model, datastore=datastore, max_results=3)\n",
        "x = 0\n",
        "\n",
        "# Iterate over each item in the JSON data with tqdm\n",
        "with tqdm(data, desc=\"Processing\") as pbar:\n",
        "    for item in pbar:\n",
        "        # Extract answers and input for each item\n",
        "        answer_set = item['output']['answer_set']\n",
        "        condition = item['input'] + \"?\"\n",
        "        lm_response = rag_LM.generate(condition=condition, max_new_tokens=30, temperature=0.7, num_return_sequences=1)\n",
        "        outputs.append(lm_response[0])\n",
        "        answer_set_list.append(RAGEvalContext(answer_set))\n",
        "        if x == 50:\n",
        "            break\n",
        "        x += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eeee9ba0",
      "metadata": {},
      "outputs": [],
      "source": [
        "evaluator = RAGEvaluator()\n",
        "f1 = evaluator.evaluate_batch(hyps=outputs, contexts=answer_set_list, metric=\"f1\", show_progress=True)\n",
        "acc = evaluator.evaluate_batch(hyps=outputs, contexts=answer_set_list, metric=\"accuracy\", show_progress=True)\n",
        "em_acc = evaluator.evaluate_batch(hyps=outputs, contexts=answer_set_list, metric=\"exact_match\", show_progress=True)\n",
        "rouge = evaluator.evaluate_batch(hyps=outputs, contexts=answer_set_list, metric=\"rougel\", show_progress=True)\n",
        "\n",
        "mean_f1 = statistics.mean(f1)\n",
        "mean_acc = statistics.mean(acc)\n",
        "mean_em_acc = statistics.mean(em_acc)\n",
        "mean_rouge = statistics.mean(rouge)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dcfc5662",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Mean F1:\", mean_f1)\n",
        "print(\"Mean Accuracy:\", mean_acc)\n",
        "print(\"Mean Exact Match Accuracy:\", mean_em_acc)\n",
        "print(\"Mean ROUGE-L:\", mean_rouge)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
