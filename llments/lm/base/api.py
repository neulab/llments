"""Base class for API-Based Language Models."""

import os
import abc
from llments.lm.lm import LanguageModel
from litellm import completion, batch_completion, ModelResponse

class APIBasedLM():
    """Base class for API-Based Language Models.

    Represents a language model that interacts with an API for generating responses.

    Usage:
    - Instantiate this class with the model name.
    - Set the API key of the language model as an environment
      variable for secure access.

    Attributes:
        model_name (str): The name of the language model.
    """

    @abc.abstractmethod
    def calculate_probability(self, condition: str | None, output: str) -> float:
        """Calculate the probability of an output given the language model.

        Args:
            condition: The conditioning sequence for the output.
                If None, the output is not conditioned.
            output: The output sequence for which the probability is calculated.

        Returns:
            float: The probability of output x given the language model.
        """
        raise NotImplementedError

    @abc.abstractmethod
    def __init__(self, model_name: str) -> None:
        """Initialize the APIBasedLM instance.

        Args:
            model_name (str): The name of the language model.
        """
        self.model_name = model_name

    @abc.abstractmethod
    def generate(
        self,
        message: str,
        condition: str | None,
        do_sample: bool = False,
        max_length: int | None = None,
        max_new_tokens: int | None = None,
        temperature: float = 1.0,
        num_return_sequences: int = 1
        ) -> list[str]:
        """Generate a response based on the given prompt.

        This method sends a prompt to the language model API and retrieves
        the generated response.
        To handle potential rate limitations, this method uses the asynchronous
        version of the completion function, which allows for concurrent function calls.

        Args:
            temperature (float): The sampling temperature to be used, between 0 and 2.
            max_tokens (float): The maximum number of tokens to generate in the chat completion.
            n (int): The number of chat completion choices to generate for each input message.
            message (str): The prompt for generating a response.

        Returns:
            ModelResponse: The generated response object from the language model.
        """
        responses = []
        response = completion(
            model = self.model_name,
            temperature = temperature,
            max_tokens = max_new_tokens,
            n = num_return_sequences,
            messages=[{"content": message, "role": "user"}]
        )
        for choice in response['choices']:
            responses.append(choice['message']['content'])
        return responses

    @abc.abstractmethod
    def chat_generate(
        self,
        messages: list[str],
        condition: str | None,
        do_sample: bool = False,
        max_length: int | None = None,
        max_new_tokens: int | None = None,
        temperature: float = 1.0,
        num_return_sequences: int = 1
        ) -> list[str]:
        """Generate responses to multiple prompts using the batch_completion function.

        This method sends multiple prompts to the language model API and retrieves
        the generated response for each of the prompts.

        Args:
            temperature (float): The sampling temperature to be used, between 0 and 2.
            max_tokens (float): The maximum number of tokens to generate in the chat completion.
            n (int): The number of chat completion choices to generate for each input message.
            messages (list): List of multiple prompts for generating the responses.

        Returns:
            list: List of responses generated by the language model for all the prompts.
        """
        responses = batch_completion(
            model = self.model_name,
            temperature = temperature,
            max_tokens = max_new_tokens,
            n = num_return_sequences,
            messages=[[{"content": content, "role": "user"}] for content in messages]
        )
        return [response['choices'][0]['message']['content'] for response in responses]
      
    @abc.abstractmethod
    def set_seed(self, seed: int) -> None:
        """Set the seed for the language model.

        Args:
            seed: The seed to set for the language model.
        """
        raise NotImplementedError
