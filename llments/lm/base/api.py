"""Base class for API-Based Language Models."""

import os
import abc
from llments.lm.lm import LanguageModel
from litellm import completion, batch_completion, ModelResponse

class APIBasedLM(LanguageModel):
    """Base class for API-Based Language Models.
    
    Represents a language model that interacts with an API for generating responses.
    
    Usage:
    - Instantiate this class with the model name.
    - Set the API key of the language model as an environment 
      variable for secure access.
      
    Attributes:
        model_name (str): The name of the language model.
    """
    
    @abc.abstractmethod
    def __init__(self, model_name: str) -> None:
        """Initialize the APIBasedLM instance.
        
        Args:
            model_name (str): The name of the language model.
        """
        self.model_name = model_name

    @abc.abstractmethod
    def generate(
        self, 
        condition: str | None,
        do_sample: bool = False,
        max_length: int | None = None,
        max_new_tokens: int | None = None,
        temperature: float = 1.0,
        num_return_sequences: int = 1,
        message: str
        ) -> list[str]:
        """Generate a response based on the given prompt.
        
        This method sends a prompt to the language model API and retrieves
        the generated response.
        To handle potential rate limitations, this method uses the asynchronous
        version of the completion function, which allows for concurrent function calls.
        
        Args:
            message (str): The prompt for generating a response.
            condition (str): The conditioning sequence for the output.
                If None, the output is not conditioned.
            do_sample (bool): Whether to use sampling or greedy decoding.
            max_length (int): The maximum length of the output sequence,
                (defaults to model max).
            max_new_tokens (float): The maximum number of tokens to generate in the chat completion.
            temperature (float): The sampling temperature to be used, between 0 and 2. 
            num_return_sequences (int): The number of chat completion choices to generate for each input message.
            
        Returns:
            list[str]: List of all the response choices generated by the language model.
        """
        responses = []
        response = completion(
            model = self.model_name,
            temperature = temperature,
            max_tokens = max_new_tokens,
            n = num_return_sequences,
            messages=[{"content": message, "role": "user"}]
        )
        for choice in response['choices']:
            responses.append(choice['message']['content'])
        return responses

    @abc.abstractmethod
    def generate_batch(
        self,
        condition: str | None,
        do_sample: bool = False,
        max_length: int | None = None,
        max_new_tokens: int | None = None,
        temperature: float = 1.0,
        num_return_sequences: int = 1,
        messages: list[str]
        ) -> list[str]:
        """Generate responses to multiple prompts using the batch_completion function.
        
        This method sends multiple prompts to the language model API and retrieves
        the generated response for each of the prompts.
        
        Args:
            messages (list[str]): List of prompts for generating responses.
            condition (str): The conditioning sequence for the output.
                If None, the output is not conditioned.
            do_sample (bool): Whether to use sampling or greedy decoding.
            max_length (int): The maximum length of the output sequence,
                (defaults to model max).
            max_new_tokens (float): The maximum number of tokens to generate in the chat completion.
            temperature (float): The sampling temperature to be used, between 0 and 2. 
            num_return_sequences (int): The number of chat completion choices to generate for each input message.
            
        Returns:
            list[str]: List of responses generated by the language model for all the prompts.
        """
        responses = batch_completion(
            model = self.model_name,
            temperature = temperature,
            max_tokens = max_new_tokens,
            n = num_return_sequences,
            messages=[[{"content": content, "role": "user"}] for content in messages]
        )
        return [response['choices'][0]['message']['content'] for response in responses]
